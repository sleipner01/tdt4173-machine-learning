{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import tensorflow as tf\n",
    "import random\n",
    "%matplotlib inline\n",
    "\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option('display.max_columns', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read dataset\n",
    "train_a = pd.read_parquet('A/train_targets.parquet')\n",
    "train_b = pd.read_parquet('B/train_targets.parquet')\n",
    "train_c = pd.read_parquet('C/train_targets.parquet')\n",
    "X_train_estimated_a = pd.read_parquet('A/X_train_estimated.parquet')\n",
    "X_train_estimated_b = pd.read_parquet('B/X_train_estimated.parquet')\n",
    "X_train_estimated_c = pd.read_parquet('C/X_train_estimated.parquet')\n",
    "X_train_observed_a = pd.read_parquet('A/X_train_observed.parquet')\n",
    "X_train_observed_b = pd.read_parquet('B/X_train_observed.parquet')\n",
    "X_train_observed_c = pd.read_parquet('C/X_train_observed.parquet')\n",
    "\n",
    "#add location to each sample\n",
    "train_a[\"location\"] = \"A\"\n",
    "train_b[\"location\"] = \"B\"\n",
    "train_c[\"location\"] = \"C\"\n",
    "X_train_estimated_a[\"location\"] = \"A\"\n",
    "X_train_estimated_b[\"location\"] = \"B\"\n",
    "X_train_estimated_c[\"location\"] = \"C\"\n",
    "X_train_observed_a[\"location\"] = \"A\"\n",
    "X_train_observed_b[\"location\"] = \"B\"\n",
    "X_train_observed_c[\"location\"] = \"C\"\n",
    "\n",
    "#remove extra minute 00 sample\n",
    "X_train_observed_a = X_train_observed_a.iloc[:-1,:]\n",
    "X_train_observed_b = X_train_observed_b.iloc[:-1,:]\n",
    "X_train_observed_c = X_train_observed_c.iloc[:-1,:]\n",
    "\n",
    "#add date_calc column same as date_forecast column to observed data\n",
    "X_train_observed_a.insert(0, \"date_calc\", X_train_observed_a[\"date_forecast\"])\n",
    "X_train_observed_b.insert(0, \"date_calc\", X_train_observed_b[\"date_forecast\"])\n",
    "X_train_observed_c.insert(0, \"date_calc\", X_train_observed_c[\"date_forecast\"])\n",
    "\n",
    "#concat all the samples and remove date_calc column\n",
    "X_train_raw = pd.concat([X_train_observed_a,\n",
    "                     X_train_observed_b,\n",
    "                     X_train_observed_c,\n",
    "                     X_train_estimated_a,\n",
    "                     X_train_estimated_b,\n",
    "                     X_train_estimated_c])\n",
    "\n",
    "#remove some weird artifacts from train_b target values\n",
    "train_b = pd.concat([train_b[:18690], train_b[20142:]])\n",
    "train_b[\"rolling\"] = train_b[\"pv_measurement\"].rolling(4).mean()\n",
    "train_b[\"keep\"] = train_b[\"pv_measurement\"] - train_b[\"rolling\"] != 0 + train_b[\"pv_measurement\"].apply(lambda a: a==0)\n",
    "train_b = train_b[train_b[\"keep\"]]\n",
    "train_b = train_b.iloc[:,:3]\n",
    "\n",
    "targets = pd.concat([train_a,\n",
    "                     train_b,\n",
    "                     train_c]).dropna()\n",
    "\n",
    "#X_train_raw[\"date_forecast\"] = X_train_raw[\"date_forecast\"] + pd.Timedelta(minutes=-60)\n",
    "\n",
    "features00 = X_train_raw[X_train_raw[\"date_forecast\"].apply(lambda time: time.minute == 0)].copy()\n",
    "features00[\"merge_time\"] = features00[\"date_forecast\"]\n",
    "\n",
    "\n",
    "features15 = X_train_raw[X_train_raw[\"date_forecast\"].apply(lambda time: time.minute == 15)].copy()\n",
    "features15[\"merge_time\"] = features15[\"date_forecast\"] + pd.Timedelta(minutes=-15)\n",
    "\n",
    "\n",
    "features30 = X_train_raw[X_train_raw[\"date_forecast\"].apply(lambda time: time.minute == 30)].copy()\n",
    "features30[\"merge_time\"] = features30[\"date_forecast\"] + pd.Timedelta(minutes=-30)\n",
    "\n",
    "\n",
    "features45 = X_train_raw[X_train_raw[\"date_forecast\"].apply(lambda time: time.minute == 45)].copy()\n",
    "features45[\"merge_time\"] = features45[\"date_forecast\"] + pd.Timedelta(minutes=-45)\n",
    "\n",
    "\n",
    "X_train_raw[\"date_forecast\"] = X_train_raw[\"date_forecast\"] + pd.Timedelta(minutes = -60)\n",
    "features60 = X_train_raw[X_train_raw[\"date_forecast\"].apply(lambda time: time.minute == 00)].copy()\n",
    "features60[\"merge_time\"] = features60[\"date_forecast\"]\n",
    "\n",
    "dataset = targets\n",
    "dataset = dataset.rename(columns={\"time\": \"merge_time\"})\n",
    "\n",
    "#averages the features meassured at target time +00, +15, +30, +45 and +60\n",
    "def add_feature_average_00_60(dataset, f00, f15, f30, f45, f60, column_name):\n",
    "    dataset = pd.merge(\n",
    "        left=dataset,\n",
    "        right = f00[[\"location\", \"merge_time\", column_name]],\n",
    "        on=[\"location\", \"merge_time\"],\n",
    "        how=\"inner\")\n",
    "    dataset = pd.merge(\n",
    "        left=dataset,\n",
    "        right = f15[[\"location\", \"merge_time\", column_name]],\n",
    "        on=[\"location\", \"merge_time\"],\n",
    "        how=\"inner\",\n",
    "        suffixes=[\"\", \"_15\"])\n",
    "    dataset = pd.merge(\n",
    "        left=dataset,\n",
    "        right = f30[[\"location\", \"merge_time\", column_name]],\n",
    "        on=[\"location\", \"merge_time\"],\n",
    "        how=\"inner\",\n",
    "        suffixes=[\"\", \"_30\"])\n",
    "    dataset = pd.merge(\n",
    "        left=dataset,\n",
    "        right = f45[[\"location\", \"merge_time\", column_name]],\n",
    "        on=[\"location\", \"merge_time\"],\n",
    "        how=\"inner\",\n",
    "        suffixes=[\"\", \"_45\"])\n",
    "    dataset = pd.merge(\n",
    "        left=dataset,\n",
    "        right = f60[[\"location\", \"merge_time\", column_name]],\n",
    "        on=[\"location\", \"merge_time\"],\n",
    "        how=\"inner\",\n",
    "        suffixes=[\"\", \"_60\"])\n",
    "\n",
    "\n",
    "    dataset[column_name] = (dataset[column_name] +\n",
    "                            dataset[column_name + \"_15\"] +\n",
    "                            dataset[column_name + \"_30\"] +\n",
    "                            dataset[column_name + \"_45\"] +\n",
    "                            dataset[column_name + \"_60\"])/5\n",
    "    dataset = dataset.drop([column_name + \"_15\",\n",
    "                            column_name + \"_30\",\n",
    "                            column_name + \"_45\",\n",
    "                            column_name + \"_60\"],\n",
    "                           axis=1)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "#adds a single feature from one observation\n",
    "def add_feature(dataset, f, column_name):\n",
    "  return pd.merge(\n",
    "        left=dataset,\n",
    "        right=f[[\"location\", \"merge_time\", column_name]],\n",
    "        on=[\"location\", \"merge_time\"],\n",
    "        how=\"inner\"\n",
    "  )\n",
    "\n",
    "#adds an One Hot Encoding of the column to the dataset\n",
    "def OHE(dataset, f, column_name, suffix=\"\"):\n",
    "\n",
    "    dataset = pd.merge(\n",
    "        left=dataset,\n",
    "        right = f[[\"location\", \"merge_time\", column_name]],\n",
    "        on=[\"location\", \"merge_time\"],\n",
    "        how=\"inner\")\n",
    "\n",
    "    values = dataset[column_name].unique()\n",
    "\n",
    "    for value in values:\n",
    "        dataset[column_name + \"_\" + suffix + str(value)] = dataset[column_name].apply(lambda a : a == value).map({True: 1, False: 0})\n",
    "\n",
    "    dataset = dataset.drop([column_name], axis=1)\n",
    "    return dataset\n",
    "\n",
    "def OHE_all(dataset, f00, f15, f30, f45, f60, column_name):\n",
    "    dataset = OHE(dataset, f00, column_name, suffix=\"00_\")\n",
    "    dataset = OHE(dataset, f15, column_name, suffix=\"15_\")\n",
    "    dataset = OHE(dataset, f30, column_name, suffix=\"30_\")\n",
    "    dataset = OHE(dataset, f45, column_name, suffix=\"45_\")\n",
    "    dataset = OHE(dataset, f60, column_name, suffix=\"60_\")\n",
    "\n",
    "    return dataset\n",
    "\n",
    "#adds all observations\n",
    "def add_all(dataset, f00, f15, f30, f45, f60, column_name):\n",
    "    dataset[column_name + \"_00\"] = add_feature(dataset, f00, column_name)[column_name]\n",
    "    dataset[column_name + \"_15\"] = add_feature(dataset, f15, column_name)[column_name]\n",
    "    dataset[column_name + \"_30\"] = add_feature(dataset, f30, column_name)[column_name]\n",
    "    dataset[column_name + \"_45\"] = add_feature(dataset, f45, column_name)[column_name]\n",
    "    dataset[column_name + \"_60\"] = add_feature(dataset, f60, column_name)[column_name]\n",
    "\n",
    "    return dataset\n",
    "\n",
    "# ADD FEATURES\n",
    "\n",
    "\n",
    "# SNOW AND PRECIPITATION\n",
    "\n",
    "# tar verdi fra +60 siden den viser måling mellom 00 og 60, #!kan det være gunstig å ha med 3h, 6h, 12h, 24h????\n",
    "dataset = add_feature(dataset, features60, \"fresh_snow_1h:cm\")\n",
    "# tar alle verdier siden måleintervallet er så kort\n",
    "dataset = add_all(dataset, features00, features15, features30, features45, features60, \"precip_5min:mm\")\n",
    "dataset = OHE_all(dataset, features00, features15, features30, features45, features60, \"precip_type_5min:idx\")\n",
    "# OHE because value is binary\n",
    "dataset = OHE(dataset, features60, \"snow_density:kgm3\")\n",
    "# disse tar jeg bare gjennomsnittet av\n",
    "dataset = add_feature_average_00_60(dataset, features00, features15, features30, features45, features60, \"snow_depth:cm\")\n",
    "dataset = add_feature_average_00_60(dataset, features00, features15, features30, features45, features60, \"snow_melt_10min:mm\")\n",
    "dataset = add_feature_average_00_60(dataset, features00, features15, features30, features45, features60, \"snow_water:kgm2\")\n",
    "\n",
    "# Virker som denne er feil (Alltid 0 bortsett fra sånn 1 verdi)\n",
    "# dataset = add_feature_average_00_60(dataset, features00, features15, features30, features45, features60, \"snow_drift:idx\")\n",
    "\n",
    "\n",
    "# ACCUMULATIVE FEATURES\n",
    "\n",
    "# tar verdi fra +60 siden den viser måling mellom 00 og 60\n",
    "dataset = add_feature(dataset, features60, \"diffuse_rad_1h:J\")\n",
    "# tar verdi fra +60 siden den viser måling mellom 00 og 60\n",
    "dataset = add_feature(dataset, features60, \"direct_rad_1h:J\")#!Try without\n",
    "\n",
    "\n",
    "# PRESSURE\n",
    "\n",
    "# tar gjennomsnittet da dette er punktmålinger ##kan hende denne burde kjøres per kvarter\n",
    "dataset = add_feature_average_00_60(dataset, features00, features15, features30, features45, features60, \"msl_pressure:hPa\")\n",
    "\n",
    "\n",
    "# TEMPERATURE\n",
    "\n",
    "# gjennomsnitt siden variasjonen hvert kvarter sannsynligvis er lav? ##kan hende denne burde kjøres per kvarter\n",
    "dataset = add_feature_average_00_60(dataset, features00, features15, features30, features45, features60, \"t_1000hPa:K\")\n",
    "\n",
    "\n",
    "# SUN\n",
    "\n",
    "#legger til alle siden har testing har vist at disse er svært viktige\n",
    "dataset = add_all(dataset, features00, features15, features30, features45, features60, \"sun_azimuth:d\")\n",
    "dataset = add_all(dataset, features00, features15, features30, features45, features60, \"sun_elevation:d\")\n",
    "\n",
    "\n",
    "# DAY AND SHADOW\n",
    "\n",
    "#tar alle verdiene siden disse nok er ekstremt viktige for modellen\n",
    "dataset = add_all(dataset, features00, features15, features30, features45, features60, \"is_day:idx\")\n",
    "#tar alle verdiene siden disse nok er ekstremt viktige for modellen\n",
    "dataset = add_all(dataset, features00, features15, features30, features45, features60, \"is_in_shadow:idx\")\n",
    "#gjennomsnitt fordi jeg vet ikke\n",
    "dataset = add_feature_average_00_60(dataset, features00, features15, features30, features45, features60, \"visibility:m\")\n",
    "\n",
    "\n",
    "# CLOUD COVER\n",
    "\n",
    "#gjennomsnitt fordi verdien er trolig momentan\n",
    "dataset = add_feature_average_00_60(dataset, features00, features15, features30, features45, features60, \"effective_cloud_cover:p\")\n",
    "\n",
    "\n",
    "# HUMIDITY AND RIME\n",
    "\n",
    "# OHE av kategorisk variabel #!Opp til diskusjon om man skal ta gjennomsnitt eller flere av målingene\n",
    "dataset = OHE(dataset, features60, \"dew_or_rime:idx\")\n",
    "#tar gjennomsnitt fordi jeg vet ikke #!diskuter\n",
    "dataset = add_feature_average_00_60(dataset, features00, features15, features30, features45, features60, \"prob_rime:p\")\n",
    "#tar gjennomsnitt fordi jeg vet ikke #!diskuter\n",
    "dataset = add_feature_average_00_60(dataset, features00, features15, features30, features45, features60, \"relative_humidity_1000hPa:p\")\n",
    "\n",
    "\n",
    "# WIND\n",
    "\n",
    "# Gjennomsnitt fordi lite variabel\n",
    "dataset = add_feature_average_00_60(dataset, features00, features15, features30, features45, features60, \"wind_speed_u_10m:ms\")\n",
    "dataset = add_feature_average_00_60(dataset, features00, features15, features30, features45, features60, \"wind_speed_v_10m:ms\")\n",
    "\n",
    "\n",
    "# OTHERS (Up for discussion)\n",
    "\n",
    "# Gjennomsnitt fordi alle er samme verdi\n",
    "dataset = add_feature_average_00_60(dataset, features00, features15, features30, features45, features60, \"elevation:m\")\n",
    "# Gjennomsnitt fordi?\n",
    "dataset = add_feature_average_00_60(dataset, features00, features15, features30, features45, features60, \"super_cooled_liquid_water:kgm2\")\n",
    "# Gjennomsnitt fordi?\n",
    "dataset = add_feature_average_00_60(dataset, features00, features15, features30, features45, features60, \"wind_speed_10m:ms\")\n",
    "# OHE fordi kun 3 mulige verdier\n",
    "dataset = OHE(dataset, features00, features15, features30, features45, features60, \"wind_speed_w_1000hPa:ms\")\n",
    "\n",
    "\"\"\"\n",
    "CURRENT FEATURES:\n",
    "\n",
    "fresh_snow_1h:cm\n",
    "precip_5min:mm\n",
    "precip_type_5min:idx\n",
    "snow_density:kgm3\n",
    "snow_depth:cm\n",
    "snow_melt_10min:mm\n",
    "snow_water:kgm2\n",
    "diffuse_rad_1h:J\n",
    "direct_rad_1h:J\n",
    "msl_pressure:hPa\n",
    "t_1000hPa:K\n",
    "sun_azimuth:d\n",
    "sun_elevation:d\n",
    "is_day:idx\n",
    "is_in_shadow:idx\n",
    "visibility:m\n",
    "effective_cloud_cover:p\n",
    "dew_or_rime:idx\n",
    "prob_rime:p\n",
    "relative_humidity_1000hPa:p\n",
    "wind_speed_u_10m:ms\n",
    "wind_speed_v_10m:ms\n",
    "elevation:m\n",
    "super_cooled_liquid_water:kgm2\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#OHE av location\n",
    "dataset[\"location_A\"] = dataset[\"location\"].apply(lambda loc: loc == \"A\").map({True: 1, False: 0})\n",
    "dataset[\"location_B\"] = dataset[\"location\"].apply(lambda loc: loc == \"B\").map({True: 1, False: 0})\n",
    "dataset[\"location_C\"] = dataset[\"location\"].apply(lambda loc: loc == \"C\").map({True: 1, False: 0})\n",
    "\n",
    "#dataset[\"day\"] = dataset[\"merge_time\"].apply(lambda a : a.day_of_year)\n",
    "#dataset[\"hour\"] = dataset[\"merge_time\"].apply(lambda a : a.hour)\n",
    "\n",
    "display(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"absolute_humidity_2m:gm3\"] = dataset[\"absolute_humidity_2m:gm3\"]/dataset[\"absolute_humidity_2m:gm3\"].std()\n",
    "dataset[\"clear_sky_energy_1h:J\"] = dataset[\"clear_sky_energy_1h:J\"]/dataset[\"clear_sky_energy_1h:J\"].std()\n",
    "dataset[\"clear_sky_rad:W\"] = dataset[\"clear_sky_rad:W\"]/dataset[\"clear_sky_rad:W\"].std()\n",
    "dataset[\"dew_point_2m:K\"] = (dataset[\"dew_point_2m:K\"]-dataset[\"dew_point_2m:K\"].min())/dataset[\"dew_point_2m:K\"].std()\n",
    "dataset[\"diffuse_rad:W\"] = dataset[\"diffuse_rad:W\"]/dataset[\"diffuse_rad:W\"].std()\n",
    "dataset[\"diffuse_rad_1h:J\"] = dataset[\"diffuse_rad_1h:J\"]/dataset[\"diffuse_rad_1h:J\"].std()\n",
    "dataset[\"direct_rad:W\"] = dataset[\"direct_rad:W\"]/dataset[\"direct_rad:W\"].std()\n",
    "dataset[\"direct_rad_1h:J\"] = dataset[\"direct_rad_1h:J\"]/dataset[\"direct_rad_1h:J\"].std()\n",
    "dataset[\"effective_cloud_cover:p\"] = dataset[\"effective_cloud_cover:p\"]/dataset[\"effective_cloud_cover:p\"].std()\n",
    "dataset[\"msl_pressure:hPa\"] = (dataset[\"msl_pressure:hPa\"]-dataset[\"msl_pressure:hPa\"].min())/dataset[\"msl_pressure:hPa\"].std()\n",
    "dataset[\"prob_rime:p\"] = dataset[\"prob_rime:p\"]/dataset[\"prob_rime:p\"].std()\n",
    "dataset[\"relative_humidity_1000hPa:p\"] = (dataset[\"relative_humidity_1000hPa:p\"])/dataset[\"relative_humidity_1000hPa:p\"].std()\n",
    "\n",
    "dataset[\"sun_azimuth:d_00\"] = dataset[\"sun_azimuth:d_00\"].apply(lambda d : np.cos((d*np.pi)/180))\n",
    "dataset[\"sun_azimuth:d_00\"] = dataset[\"sun_azimuth:d_00\"]/dataset[\"sun_azimuth:d_00\"].std()\n",
    "dataset[\"sun_azimuth:d_15\"] = dataset[\"sun_azimuth:d_15\"].apply(lambda d : np.cos((d*np.pi)/180))\n",
    "dataset[\"sun_azimuth:d_15\"] = dataset[\"sun_azimuth:d_15\"]/dataset[\"sun_azimuth:d_15\"].std()\n",
    "dataset[\"sun_azimuth:d_30\"] = dataset[\"sun_azimuth:d_30\"].apply(lambda d : np.cos((d*np.pi)/180))\n",
    "dataset[\"sun_azimuth:d_30\"] = dataset[\"sun_azimuth:d_30\"]/dataset[\"sun_azimuth:d_30\"].std()\n",
    "dataset[\"sun_azimuth:d_45\"] = dataset[\"sun_azimuth:d_45\"].apply(lambda d : np.cos((d*np.pi)/180))\n",
    "dataset[\"sun_azimuth:d_45\"] = dataset[\"sun_azimuth:d_45\"]/dataset[\"sun_azimuth:d_45\"].std()\n",
    "dataset[\"sun_azimuth:d_60\"] = dataset[\"sun_azimuth:d_60\"].apply(lambda d : np.cos((d*np.pi)/180))\n",
    "dataset[\"sun_azimuth:d_60\"] = dataset[\"sun_azimuth:d_60\"]/dataset[\"sun_azimuth:d_60\"].std()\n",
    "\n",
    "dataset[\"sun_elevation:d_00\"] = dataset[\"sun_elevation:d_00\"].apply(lambda d : np.sin((d*np.pi)/180))\n",
    "dataset[\"sun_elevation:d_00\"] = dataset[\"sun_elevation:d_00\"]/dataset[\"sun_elevation:d_00\"].std()\n",
    "dataset[\"sun_elevation:d_15\"] = dataset[\"sun_elevation:d_15\"].apply(lambda d : np.sin((d*np.pi)/180))\n",
    "dataset[\"sun_elevation:d_15\"] = dataset[\"sun_elevation:d_15\"]/dataset[\"sun_elevation:d_15\"].std()\n",
    "dataset[\"sun_elevation:d_30\"] = dataset[\"sun_elevation:d_30\"].apply(lambda d : np.sin((d*np.pi)/180))\n",
    "dataset[\"sun_elevation:d_30\"] = dataset[\"sun_elevation:d_30\"]/dataset[\"sun_elevation:d_30\"].std()\n",
    "dataset[\"sun_elevation:d_45\"] = dataset[\"sun_elevation:d_45\"].apply(lambda d : np.sin((d*np.pi)/180))\n",
    "dataset[\"sun_elevation:d_45\"] = dataset[\"sun_elevation:d_45\"]/dataset[\"sun_elevation:d_45\"].std()\n",
    "dataset[\"sun_elevation:d_60\"] = dataset[\"sun_elevation:d_60\"].apply(lambda d : np.sin((d*np.pi)/180))\n",
    "dataset[\"sun_elevation:d_60\"] = dataset[\"sun_elevation:d_60\"]/dataset[\"sun_elevation:d_60\"].std()\n",
    "\n",
    "dataset[\"t_1000hPa:K\"] = (dataset[\"t_1000hPa:K\"]-dataset[\"t_1000hPa:K\"].min())/dataset[\"t_1000hPa:K\"].std()\n",
    "dataset[\"total_cloud_cover:p\"] = dataset[\"total_cloud_cover:p\"]/dataset[\"total_cloud_cover:p\"].std()\n",
    "dataset[\"visibility:m\"] = dataset[\"visibility:m\"]/dataset[\"visibility:m\"].std()\n",
    "dataset[\"wind_speed_u_10m:ms\"] = dataset[\"wind_speed_u_10m:ms\"]/dataset[\"wind_speed_u_10m:ms\"].std()\n",
    "dataset[\"wind_speed_v_10m:ms\"] = dataset[\"wind_speed_v_10m:ms\"]/dataset[\"wind_speed_v_10m:ms\"].std()\n",
    "#dataset[\"day\"] = 10*dataset[\"day\"]/dataset[\"day\"].std()\n",
    "#dataset[\"hour\"] = dataset[\"hour\"]/dataset[\"hour\"].std()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.sort_values(by=\"merge_time\")\n",
    "\n",
    "datasetX = dataset.iloc[:, 3:]\n",
    "datasetY = dataset.iloc[:, 1]\n",
    "\n",
    "display(datasetX)\n",
    "display(datasetY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!ReWrite\n",
    "def evaluate_models(models, X, Y):\n",
    "    preds = X.iloc[:,1:2]\n",
    "\n",
    "    for i in range(len(models)):\n",
    "        preds[str(i)] = models[i].predict(X)\n",
    "\n",
    "\n",
    "    preds = preds.iloc[:,1:]\n",
    "\n",
    "\n",
    "    preds[\"final\"] = preds.mean(axis=1)\n",
    "    preds[\"losses\"] = (preds[\"final\"] - Y).apply(lambda a : np.abs(a))\n",
    "    return preds[\"losses\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(models, X):\n",
    "    preds = X.iloc[:,1:2]\n",
    "\n",
    "    for i in range(len(models)):\n",
    "        preds[str(i)] = models[i].predict(X, verbose=0)\n",
    "\n",
    "\n",
    "    preds = preds.iloc[:,1:]\n",
    "    return preds.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_folds = 9\n",
    "\n",
    "losses = []\n",
    "models = []\n",
    "for f in range(0, num_folds):\n",
    "  evalIndex=f*10000\n",
    "  evalSize=10000\n",
    "\n",
    "  #partition into training and evalset\n",
    "  trainsetX = pd.concat([datasetX.iloc[:evalIndex,:],datasetX.iloc[evalIndex+evalSize:,:]])\n",
    "  trainsetY = pd.concat([datasetY.iloc[:evalIndex],datasetY.iloc[evalIndex+evalSize:]])\n",
    "  evalsetX = datasetX.iloc[evalIndex:evalIndex+evalSize,:]\n",
    "  evalsetY = datasetY.iloc[evalIndex:evalIndex+evalSize]\n",
    "\n",
    "  numModels = 20\n",
    "\n",
    "  models = []\n",
    "\n",
    "\n",
    "\n",
    "  i = 0\n",
    "  while(i < numModels):\n",
    "    if(i < 10):\n",
    "      models.append(tf.keras.models.Sequential([\n",
    "            #tf.keras.layers.GaussianNoise(stddev=0.1, seed=42),\n",
    "            tf.keras.layers.Dense(70, activation=\"tanh\",\n",
    "              kernel_initializer=tf.keras.initializers.RandomUniform(-1, 1),\n",
    "                                  bias_initializer=tf.keras.initializers.Zeros()),\n",
    "            tf.keras.layers.Dense(70, activation=\"relu\",\n",
    "              kernel_initializer=tf.keras.initializers.GlorotNormal()),\n",
    "            tf.keras.layers.Dense(1, activation=\"relu\",\n",
    "              kernel_initializer=tf.keras.initializers.GlorotNormal()),\n",
    "        ]))\n",
    "    else:\n",
    "      models.append(tf.keras.models.Sequential([\n",
    "            #tf.keras.layers.GaussianNoise(stddev=0.1, seed=42),\n",
    "            tf.keras.layers.Dense(100, activation=\"tanh\",\n",
    "              kernel_initializer=tf.keras.initializers.RandomUniform(-1, 1),\n",
    "                                  bias_initializer=tf.keras.initializers.Zeros()),\n",
    "            tf.keras.layers.Dense(80, activation=\"relu\",\n",
    "              kernel_initializer=tf.keras.initializers.GlorotNormal(),\n",
    "                                  bias_initializer=tf.keras.initializers.Zeros()),\n",
    "            tf.keras.layers.Dense(60, activation=\"relu\",\n",
    "              kernel_initializer=tf.keras.initializers.GlorotNormal(),\n",
    "                                  bias_initializer=tf.keras.initializers.Zeros()),\n",
    "            tf.keras.layers.Dense(40, activation=\"relu\",\n",
    "              kernel_initializer=tf.keras.initializers.GlorotNormal(),\n",
    "                                  bias_initializer=tf.keras.initializers.Zeros()),\n",
    "            tf.keras.layers.Dense(20, activation=\"relu\",\n",
    "              kernel_initializer=tf.keras.initializers.GlorotNormal(),\n",
    "                                  bias_initializer=tf.keras.initializers.Zeros()),\n",
    "            tf.keras.layers.Dense(1, activation=\"relu\",\n",
    "              kernel_initializer=tf.keras.initializers.GlorotNormal(),\n",
    "                                  bias_initializer=tf.keras.initializers.Zeros()),\n",
    "        ]))\n",
    "    models[i].compile(\n",
    "        optimizer=tf.keras.optimizers.experimental.Adadelta(learning_rate=1,\n",
    "                                                            #weight_decay=0.0001\n",
    "                                                            ),\n",
    "        loss=\"mean_absolute_error\"\n",
    "    )\n",
    "\n",
    "    history = models[i].fit(\n",
    "                        x = trainsetX.sample(frac=0.8 if i < 10 else 0.8, random_state=i),\n",
    "                        y = trainsetY.sample(frac=0.8 if i < 10 else 0.8, random_state=i),\n",
    "                        batch_size = 1000,\n",
    "                        epochs = 1,\n",
    "                        verbose = 0,\n",
    "                        #validation_data = [evalsetX, evalsetY]\n",
    "                    )\n",
    "\n",
    "    #retry if model is nonsensical\n",
    "    loss = models[i].evaluate(evalsetX, evalsetY, verbose=0)\n",
    "    if(loss < evalsetY.mean() + 4 and loss > evalsetY.mean() - 4):\n",
    "      print(\"discarding\")\n",
    "      del models[i]\n",
    "      continue\n",
    "\n",
    "    history = models[i].fit(\n",
    "                        x = trainsetX.sample(frac=0.65 if i < 10 else 0.65, random_state=i),\n",
    "                        y = trainsetY.sample(frac=0.65 if i < 10 else 0.65, random_state=i),\n",
    "                        batch_size = 1000,\n",
    "                        epochs = 40,\n",
    "                        verbose = 0,\n",
    "                        validation_data = [evalsetX, evalsetY]\n",
    "                    )\n",
    "    loss = models[i].evaluate(evalsetX, evalsetY)\n",
    "    print(i, \" complete\")\n",
    "    i+=1\n",
    "\n",
    "  loss = evaluate_models(models, evalsetX, evalsetY)\n",
    "  print(loss)\n",
    "  losses.append(loss)\n",
    "\n",
    "print(losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = get_predictions(models, evalsetX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "comparison = dataset[[\"location\", \"merge_time\", \"pv_measurement\"]]\n",
    "comparison = comparison.iloc[20000:30000,:]\n",
    "comparison[\"pv_pred\"] = preds\n",
    "comparison = comparison.rename(columns={\"pv_measurement\": \"pv_true\"})\n",
    "\n",
    "comparisonA = comparison[comparison[\"location\"].apply(lambda loc: loc == \"A\")]\n",
    "comparisonB = comparison[comparison[\"location\"].apply(lambda loc: loc == \"B\")]\n",
    "comparisonC = comparison[comparison[\"location\"].apply(lambda loc: loc == \"C\")]\n",
    "\n",
    "display(comparison.describe())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, 1, figsize=(20, 20), sharex=True)\n",
    "\n",
    "comparisonA.iloc[-400:][['merge_time', 'pv_true']].set_index('merge_time').plot(ax=axs[0], title='A', color='blue')\n",
    "comparisonA.iloc[-400:][['merge_time', \"pv_pred\"]].set_index('merge_time').plot(ax=axs[0], title='A', color='red')\n",
    "comparisonB.iloc[-400:][['merge_time', 'pv_true']].set_index('merge_time').plot(ax=axs[1], title='B', color='blue')\n",
    "comparisonB.iloc[-400:][['merge_time', \"pv_pred\"]].set_index('merge_time').plot(ax=axs[1], title='B', color='red')\n",
    "comparisonC.iloc[-400:][['merge_time', 'pv_true']].set_index('merge_time').plot(ax=axs[2], title='C', color='blue')\n",
    "comparisonC.iloc[-400:][['merge_time', \"pv_pred\"]].set_index('merge_time').plot(ax=axs[2], title='C', color='red')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
