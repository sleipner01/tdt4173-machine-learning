{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import tensorflow as tf\n",
    "%matplotlib inline\n",
    "\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option('display.max_columns', 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read dataset\n",
    "train_a = pd.read_parquet('A/train_targets.parquet')\n",
    "train_b = pd.read_parquet('B/train_targets.parquet')\n",
    "train_c = pd.read_parquet('C/train_targets.parquet')\n",
    "X_train_estimated_a = pd.read_parquet('A/X_train_estimated.parquet')\n",
    "X_train_estimated_b = pd.read_parquet('B/X_train_estimated.parquet')\n",
    "X_train_estimated_c = pd.read_parquet('C/X_train_estimated.parquet')\n",
    "X_train_observed_a = pd.read_parquet('A/X_train_observed.parquet')\n",
    "X_train_observed_b = pd.read_parquet('B/X_train_observed.parquet')\n",
    "X_train_observed_c = pd.read_parquet('C/X_train_observed.parquet')\n",
    "\n",
    "#add location to each sample\n",
    "train_a[\"location\"] = \"A\"\n",
    "train_b[\"location\"] = \"B\"\n",
    "train_c[\"location\"] = \"C\"\n",
    "X_train_estimated_a[\"location\"] = \"A\"\n",
    "X_train_estimated_b[\"location\"] = \"B\"\n",
    "X_train_estimated_c[\"location\"] = \"C\"\n",
    "X_train_observed_a[\"location\"] = \"A\"\n",
    "X_train_observed_b[\"location\"] = \"B\"\n",
    "X_train_observed_c[\"location\"] = \"C\"\n",
    "\n",
    "#remove extra minute 00 sample\n",
    "X_train_observed_a = X_train_observed_a.iloc[:-1,:]\n",
    "X_train_observed_b = X_train_observed_b.iloc[:-1,:]\n",
    "X_train_observed_c = X_train_observed_c.iloc[:-1,:]\n",
    "\n",
    "#add date_calc column same as date_forecast column to observed data\n",
    "X_train_observed_a.insert(0, \"date_calc\", X_train_observed_a[\"date_forecast\"])\n",
    "X_train_observed_b.insert(0, \"date_calc\", X_train_observed_b[\"date_forecast\"])\n",
    "X_train_observed_c.insert(0, \"date_calc\", X_train_observed_c[\"date_forecast\"])\n",
    "\n",
    "#concat all the samples and remove date_calc column\n",
    "X_train_raw = pd.concat([X_train_observed_a,\n",
    "                     X_train_observed_b,\n",
    "                     X_train_observed_c,\n",
    "                     X_train_estimated_a,\n",
    "                     X_train_estimated_b,\n",
    "                     X_train_estimated_c])\n",
    "\n",
    "#feature indicating time between date_calc and date_forecast\n",
    "X_train_raw[\"calc_time\"] =(X_train_raw[\"date_forecast\"] - X_train_raw[\"date_calc\"]).astype('timedelta64[s]')\n",
    "\n",
    "#fill nans\n",
    "X_train_raw[\"snow_density:kgm3\"] = X_train_raw[\"snow_density:kgm3\"].apply(\n",
    "    lambda a : np.isnan(a)\n",
    "    ).map({True: 0, False: 1})\n",
    "X_train_raw[\"ceiling_height_agl:m\"] = X_train_raw[\"ceiling_height_agl:m\"].apply(\n",
    "    lambda a : -1000 if np.isnan(a) else a\n",
    ")\n",
    "X_train_raw[\"cloud_base_agl:m\"] = X_train_raw[\"ceiling_height_agl:m\"].apply(\n",
    "    lambda a : -1000 if np.isnan(a) else a\n",
    ")\n",
    "\n",
    "#create seperate dataframes for measurments at minute 00, 15, 30 and 45\n",
    "X_train00 = X_train_raw[X_train_raw[\"date_forecast\"].apply(lambda time: time.minute == 0)].reset_index().iloc[:,1:]\n",
    "X_train15 = X_train_raw[X_train_raw[\"date_forecast\"].apply(lambda time: time.minute == 15)].reset_index().iloc[:,1:]\n",
    "X_train30 = X_train_raw[X_train_raw[\"date_forecast\"].apply(lambda time: time.minute == 30)].reset_index().iloc[:,1:]\n",
    "X_train45 = X_train_raw[X_train_raw[\"date_forecast\"].apply(lambda time: time.minute == 45)].reset_index().iloc[:,1:]\n",
    "\n",
    "#remove redundant data\n",
    "X_train15 = X_train15.iloc[:,2:-2]\n",
    "X_train30 = X_train30.iloc[:,2:-2]\n",
    "X_train45 = X_train45.iloc[:,2:-2]\n",
    "\n",
    "#join observations into single sample\n",
    "X_train = X_train00.join(X_train15, lsuffix=\"_00\", rsuffix=\"_15\").join(X_train30.join(X_train45, lsuffix=\"_30\", rsuffix=\"_45\"))\n",
    "\n",
    "#rename column for merging with targets\n",
    "X_train = X_train.rename(columns={\"date_forecast\" : \"time\"})\n",
    "\n",
    "#concat target values and drop NaN values\n",
    "targets = pd.concat([train_a,\n",
    "                     train_b,\n",
    "                     train_c]).dropna()\n",
    "\n",
    "#merge weatherfeatures with corresponding target pv measurement\n",
    "dataset = pd.merge(X_train, targets, how=\"right\", on=[\"time\", \"location\"])\n",
    "\n",
    "#shuffle dataset\n",
    "dataset = dataset.sample(frac=1, random_state=43).reset_index().iloc[:,1:]\n",
    "\n",
    "#split into features and targets\n",
    "datasetX = dataset.iloc[:, :-1]\n",
    "datasetY = dataset.iloc[:, -1:]\n",
    "\n",
    "#add day_of_year and hour feature columns\n",
    "datasetX[\"day\"] = datasetX[\"time\"].dt.day_of_year\n",
    "datasetX[\"hour\"] = datasetX[\"time\"].dt.hour\n",
    "\n",
    "#get indexes of samples in the months of the test dataset\n",
    "indexMayJuneJuly = datasetX[\"time\"].apply(lambda time : time.month in [5, 6, 7])\n",
    "\n",
    "#OHE encoding for catagorical feature \"location\"\n",
    "datasetX[\"location_A\"] = datasetX[\"location\"].apply(lambda a : a == \"A\").map({True: 1, False: 0})\n",
    "datasetX[\"location_B\"] = datasetX[\"location\"].apply(lambda a : a == \"B\").map({True: 1, False: 0})\n",
    "datasetX[\"location_C\"] = datasetX[\"location\"].apply(lambda a : a == \"C\").map({True: 1, False: 0})\n",
    "\n",
    "datasetX = datasetX.drop(\"location\", axis=1)\n",
    "\n",
    "#move datecalc column\n",
    "#date_calc_column = datasetX.pop(\"date_calc\")\n",
    "#datasetX.insert(183, \"is_not_calculated\", date_calc_column) #!constant 183 can be source of bugs\n",
    "\n",
    "#fix nans for some reason???\n",
    "#datasetX[\"is_not_calculated\"] = datasetX[\"is_not_calculated\"].fillna(method=\"ffill\")\n",
    "\n",
    "#map location labels to numbers\n",
    "#datasetX[\"location\"] = datasetX[\"location\"].map({\"A\": 0, \"B\": 1, \"C\": 2})\n",
    "\n",
    "#drop time and date_calc columns\n",
    "datasetX = datasetX.iloc[:,2:]\n",
    "\n",
    "#calculate mean and std for normalizing data, values should also be used for normalizing test data\n",
    "dataMean = datasetX.mean()\n",
    "dataStd = datasetX.std()\n",
    "\n",
    "#normalize data\n",
    "datasetX.iloc[:,:-4] = ((datasetX.iloc[:,:-4]-dataMean[:-4])/dataStd[:-4]).fillna(value=0)\n",
    "\n",
    "\n",
    "#partition into training and evalset\n",
    "trainsetX = datasetX.iloc[:85000,:]\n",
    "trainsetY = datasetY.iloc[:85000,:]\n",
    "trainsetIndexMayJuneJuly = indexMayJuneJuly[:85000]\n",
    "evalsetX = datasetX.iloc[85000:,:]\n",
    "evalsetY = datasetY.iloc[85000:,:]\n",
    "evalsetIndexMayJuneJuly = indexMayJuneJuly[85000:]\n",
    "\n",
    "display(datasetX)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
