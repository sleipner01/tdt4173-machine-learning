{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option('display.max_columns', 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_a = pd.read_parquet('A/train_targets.parquet')\n",
    "y_b = pd.read_parquet('B/train_targets.parquet')\n",
    "y_c = pd.read_parquet('C/train_targets.parquet')\n",
    "\n",
    "X_a = pd.read_parquet('A/X_train_observed.parquet')\n",
    "X_b = pd.read_parquet('B/X_train_observed.parquet')\n",
    "X_c = pd.read_parquet('C/X_train_observed.parquet')\n",
    "\n",
    "X_a_estimated = pd.read_parquet('A/X_train_estimated.parquet')\n",
    "X_b_estimated = pd.read_parquet('B/X_train_estimated.parquet')\n",
    "X_c_estimated = pd.read_parquet('C/X_train_estimated.parquet')\n",
    "\n",
    "X_a_test = pd.read_parquet('A/X_test_estimated.parquet')\n",
    "X_b_test = pd.read_parquet('B/X_test_estimated.parquet')\n",
    "X_c_test = pd.read_parquet('C/X_test_estimated.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove highly correlated features\n",
    "\n",
    "Investigated in **correlation_cleanup.ipynb**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = ['fresh_snow_12h:cm', 'fresh_snow_24h:cm', 'fresh_snow_3h:cm', 'fresh_snow_6h:cm', 'diffuse_rad:W', 'direct_rad:W', 'pressure_100m:hPa', 'pressure_50m:hPa', 'sfc_pressure:hPa', 'absolute_humidity_2m:gm3','air_density_2m:kgm3','dew_point_2m:K', 'clear_sky_rad:W', 'sun_elevation:d', 'clear_sky_energy_1h:J', 'is_in_shadow:idx', 'total_cloud_cover:p']\n",
    "\n",
    "X_a.drop(to_drop, axis=1, inplace=True)\n",
    "X_b.drop(to_drop, axis=1, inplace=True)\n",
    "X_c.drop(to_drop, axis=1, inplace=True)\n",
    "\n",
    "X_a_estimated.drop(to_drop, axis=1, inplace=True)\n",
    "X_b_estimated.drop(to_drop, axis=1, inplace=True)\n",
    "X_c_estimated.drop(to_drop, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fix missing values\n",
    "\n",
    "### Fix snow density\n",
    "\n",
    "The snow density feature is either 250 or Nan, so I'll map it to a binary. 0 for NaN values and 1 for the 250 values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_snow_density(df):\n",
    "    df[\"snow_density:kgm3\"] = df[\"snow_density:kgm3\"].apply(\n",
    "        lambda a : np.isnan(a)\n",
    "        ).map({True: 0, False: 1})\n",
    "\n",
    "fix_snow_density(X_a)\n",
    "fix_snow_density(X_b)\n",
    "fix_snow_density(X_c)\n",
    "\n",
    "fix_snow_density(X_a_estimated)\n",
    "fix_snow_density(X_b_estimated)\n",
    "fix_snow_density(X_c_estimated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix the rest\n",
    "\n",
    "We'll fix the two other features with missing values using multiple imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiple_imputation(df):\n",
    "    # Create a MICE imputer    \n",
    "    mi = sm.MICE(endog=df, exog=None, nskip=1, niter=10, verbose=1)\n",
    "\n",
    "    # Fit the MICE model\n",
    "    results = mi.fit()\n",
    "\n",
    "    # Obtain the imputed datasets\n",
    "    imputed_datasets = results.endog\n",
    "\n",
    "    # Now you have multiple imputed datasets. You can analyze each separately or combine the results.\n",
    "\n",
    "    # Combine the results, e.g., by averaging or using a weighted combination\n",
    "    mean_imputation = imputed_datasets.mean(axis=0)\n",
    "\n",
    "    # Perform your analysis using the imputed datasets or the combined result"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
