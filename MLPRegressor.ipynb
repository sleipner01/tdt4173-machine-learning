{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import sklearn\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option('display.max_columns', 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_1/my8qv4fj7rx474cf5cw329lm0000gn/T/ipykernel_75017/3919423765.py:118: PerformanceWarning: Adding/subtracting object-dtype array to TimedeltaArray not vectorized.\n",
      "  datasetX.iloc[:,:-4] = ((datasetX.iloc[:,:-4]-dataMean[:-4])/dataStd[:-4]).fillna(value=0)\n"
     ]
    }
   ],
   "source": [
    "#Read dataset\n",
    "train_a = pd.read_parquet('A/train_targets.parquet')\n",
    "train_b = pd.read_parquet('B/train_targets.parquet')\n",
    "train_c = pd.read_parquet('C/train_targets.parquet')\n",
    "\n",
    "X_train_estimated_a = pd.read_parquet('A/X_train_estimated.parquet')\n",
    "X_train_estimated_b = pd.read_parquet('B/X_train_estimated.parquet')\n",
    "X_train_estimated_c = pd.read_parquet('C/X_train_estimated.parquet')\n",
    "\n",
    "X_train_observed_a = pd.read_parquet('A/X_train_observed.parquet')\n",
    "X_train_observed_b = pd.read_parquet('B/X_train_observed.parquet')\n",
    "X_train_observed_c = pd.read_parquet('C/X_train_observed.parquet')\n",
    "\n",
    "#add location to each sample\n",
    "train_a[\"location\"] = \"A\"\n",
    "train_b[\"location\"] = \"B\"\n",
    "train_c[\"location\"] = \"C\"\n",
    "\n",
    "X_train_estimated_a[\"location\"] = \"A\"\n",
    "X_train_estimated_b[\"location\"] = \"B\"\n",
    "X_train_estimated_c[\"location\"] = \"C\"\n",
    "\n",
    "X_train_observed_a[\"location\"] = \"A\"\n",
    "X_train_observed_b[\"location\"] = \"B\"\n",
    "X_train_observed_c[\"location\"] = \"C\"\n",
    "\n",
    "#remove extra minute 00 sample\n",
    "X_train_observed_a = X_train_observed_a.iloc[:-1,:]\n",
    "X_train_observed_b = X_train_observed_b.iloc[:-1,:]\n",
    "X_train_observed_c = X_train_observed_c.iloc[:-1,:]\n",
    "\n",
    "#add date_calc column same as date_forecast column to observed data\n",
    "X_train_observed_a.insert(0, \"date_calc\", X_train_observed_a[\"date_forecast\"])\n",
    "X_train_observed_b.insert(0, \"date_calc\", X_train_observed_b[\"date_forecast\"])\n",
    "X_train_observed_c.insert(0, \"date_calc\", X_train_observed_c[\"date_forecast\"])\n",
    "\n",
    "#concat all the samples\n",
    "X_train_raw = pd.concat([X_train_observed_a,\n",
    "                     X_train_observed_b,\n",
    "                     X_train_observed_c,\n",
    "                     X_train_estimated_a,\n",
    "                     X_train_estimated_b,\n",
    "                     X_train_estimated_c])\n",
    "\n",
    "#feature indicating time between date_calc and date_forecast\n",
    "X_train_raw[\"calc_time\"] =(X_train_raw[\"date_forecast\"] - X_train_raw[\"date_calc\"]).astype('timedelta64[s]')\n",
    "\n",
    "#fill nans\n",
    "X_train_raw[\"snow_density:kgm3\"] = X_train_raw[\"snow_density:kgm3\"].apply(\n",
    "    lambda a : np.isnan(a)\n",
    "    ).map({True: 0, False: 1})\n",
    "X_train_raw[\"ceiling_height_agl:m\"] = X_train_raw[\"ceiling_height_agl:m\"].apply(\n",
    "    lambda a : -1000 if np.isnan(a) else a\n",
    ")\n",
    "X_train_raw[\"cloud_base_agl:m\"] = X_train_raw[\"ceiling_height_agl:m\"].apply(\n",
    "    lambda a : -1000 if np.isnan(a) else a\n",
    ")\n",
    "\n",
    "#create seperate dataframes for measurments at minute 00, 15, 30 and 45\n",
    "X_train00 = X_train_raw[X_train_raw[\"date_forecast\"].apply(lambda time: time.minute == 0)].reset_index().iloc[:,1:]\n",
    "X_train15 = X_train_raw[X_train_raw[\"date_forecast\"].apply(lambda time: time.minute == 15)].reset_index().iloc[:,1:]\n",
    "X_train30 = X_train_raw[X_train_raw[\"date_forecast\"].apply(lambda time: time.minute == 30)].reset_index().iloc[:,1:]\n",
    "X_train45 = X_train_raw[X_train_raw[\"date_forecast\"].apply(lambda time: time.minute == 45)].reset_index().iloc[:,1:]\n",
    "\n",
    "#remove redundant data\n",
    "X_train15 = X_train15.iloc[:,2:-2]\n",
    "X_train30 = X_train30.iloc[:,2:-2]\n",
    "X_train45 = X_train45.iloc[:,2:-2]\n",
    "\n",
    "#join observations into single sample\n",
    "X_train = X_train00.join(X_train15, lsuffix=\"_00\", rsuffix=\"_15\").join(X_train30.join(X_train45, lsuffix=\"_30\", rsuffix=\"_45\"))\n",
    "\n",
    "#rename column for merging with targets\n",
    "X_train = X_train.rename(columns={\"date_forecast\" : \"time\"})\n",
    "\n",
    "#concat target values and drop NaN values\n",
    "targets = pd.concat([train_a,\n",
    "                     train_b,\n",
    "                     train_c]).dropna()\n",
    "\n",
    "#merge weatherfeatures with corresponding target pv measurement\n",
    "dataset = pd.merge(X_train, targets, how=\"right\", on=[\"time\", \"location\"])\n",
    "\n",
    "#shuffle dataset\n",
    "dataset = dataset.sample(frac=1, random_state=43).reset_index().iloc[:,1:]\n",
    "\n",
    "#split into features and targets\n",
    "datasetX = dataset.iloc[:, :-1]\n",
    "datasetY = dataset.iloc[:, -1:]\n",
    "\n",
    "#add day and hour feature columns\n",
    "datasetX[\"day\"] = datasetX[\"time\"].dt.day_of_year\n",
    "datasetX[\"hour\"] = datasetX[\"time\"].dt.hour\n",
    "\n",
    "#get indexes of samples in the months of the test dataset\n",
    "indexMayJuneJuly = datasetX[\"time\"].apply(lambda time : time.month in [5, 6, 7])\n",
    "\n",
    "#OHE encoding for catagorical feature \"location\"\n",
    "\n",
    "# Do not include the data because it could overfit the model\n",
    "\"\"\"\n",
    "datasetX[\"location_A\"] = datasetX[\"location\"].apply(lambda a : a == \"A\").map({True: 1, False: 0})\n",
    "datasetX[\"location_B\"] = datasetX[\"location\"].apply(lambda a : a == \"B\").map({True: 1, False: 0})\n",
    "datasetX[\"location_C\"] = datasetX[\"location\"].apply(lambda a : a == \"C\").map({True: 1, False: 0})\n",
    "\"\"\"\n",
    "\n",
    "# Therefore also drop location column\n",
    "datasetX = datasetX.drop(\"location\", axis=1)\n",
    "\n",
    "#drop time and date_calc columns\n",
    "datasetX = datasetX.iloc[:,2:]\n",
    "\n",
    "#calculate mean and std for normalizing data, values should also be used for normalizing test data\n",
    "dataMean = datasetX.mean()\n",
    "dataStd = datasetX.std()\n",
    "\n",
    "#normalize data\n",
    "datasetX.iloc[:,:-4] = ((datasetX.iloc[:,:-4]-dataMean[:-4])/dataStd[:-4]).fillna(value=0)\n",
    "\n",
    "#partition into training and evalset\n",
    "trainsetX = datasetX.iloc[:85000,:]\n",
    "trainsetY = datasetY.iloc[:85000,:]\n",
    "trainsetIndexMayJuneJuly = indexMayJuneJuly[:85000]\n",
    "evalsetX = datasetX.iloc[85000:,:]\n",
    "evalsetY = datasetY.iloc[85000:,:]\n",
    "evalsetIndexMayJuneJuly = indexMayJuneJuly[85000:]\n",
    "\n",
    "#display(datasetX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up highly correlated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create correlation matrix\n",
    "corr_matrix = trainsetX.corr().abs()\n",
    "\n",
    "# Select upper triangle of correlation matrix\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "# Find features with correlation greater than 0.75\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.75)]\n",
    "\n",
    "trainsetX = trainsetX.drop(to_drop, axis=1)\n",
    "evalsetX = evalsetX.drop(to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import tools\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, ShuffleSplit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define function for doing the randomized search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def searchBestFeatures(regressor, parameters):\n",
    "    # define splits for CV\n",
    "    sss = ShuffleSplit(n_splits=5, test_size=0.2, random_state=43)\n",
    "\n",
    "    # define search\n",
    "    clf = RandomizedSearchCV(regressor, param_distributions=parameters ,random_state=43, verbose=3, n_iter=10, cv=sss, scoring=\"neg_mean_absolute_error\", n_jobs=-1)\n",
    "\n",
    "    # perform the search\n",
    "    search = clf.fit(trainsetX, trainsetY.values.ravel())\n",
    "\n",
    "    # report the best results\n",
    "    return search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the best hidden_layer_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/oscarmarcussen/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_search.py:307: UserWarning: The total space of parameters 2 is smaller than n_iter=10. Running 2 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END ...hidden_layer_sizes=(256, 128);, score=-96.396 total time= 4.9min\n",
      "[CV 1/5] END ...hidden_layer_sizes=(256, 128);, score=-94.304 total time= 5.3min\n",
      "[CV 2/5] END ...hidden_layer_sizes=(256, 128);, score=-95.398 total time= 5.3min\n",
      "[CV 1/5] END hidden_layer_sizes=(256, 128, 64);, score=-107.453 total time= 6.0min\n",
      "[CV 5/5] END ...hidden_layer_sizes=(256, 128);, score=-93.104 total time= 6.1min\n",
      "[CV 2/5] END hidden_layer_sizes=(256, 128, 64);, score=-100.681 total time= 6.9min\n",
      "[CV 3/5] END ...hidden_layer_sizes=(256, 128);, score=-97.665 total time= 7.2min\n",
      "[CV 3/5] END hidden_layer_sizes=(256, 128, 64);, score=-92.981 total time= 7.7min\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/oscarmarcussen/NTNU/ML/tdt4173-machine-learning/MLPRegressor.ipynb Cell 12\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/oscarmarcussen/NTNU/ML/tdt4173-machine-learning/MLPRegressor.ipynb#X10sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# parameters = {'hidden_layer_sizes':((8,), (16, 8,), (16,), (32, 16,), (32,), (64, 32), (64,))}\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/oscarmarcussen/NTNU/ML/tdt4173-machine-learning/MLPRegressor.ipynb#X10sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m parameters \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mhidden_layer_sizes\u001b[39m\u001b[39m'\u001b[39m:((\u001b[39m256\u001b[39m, \u001b[39m128\u001b[39m), (\u001b[39m256\u001b[39m, \u001b[39m128\u001b[39m, \u001b[39m64\u001b[39m))}\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/oscarmarcussen/NTNU/ML/tdt4173-machine-learning/MLPRegressor.ipynb#X10sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m results \u001b[39m=\u001b[39m searchBestFeatures(regressor, parameters)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/oscarmarcussen/NTNU/ML/tdt4173-machine-learning/MLPRegressor.ipynb#X10sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mprint\u001b[39m(results)\n",
      "\u001b[1;32m/Users/oscarmarcussen/NTNU/ML/tdt4173-machine-learning/MLPRegressor.ipynb Cell 12\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/oscarmarcussen/NTNU/ML/tdt4173-machine-learning/MLPRegressor.ipynb#X10sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m clf \u001b[39m=\u001b[39m RandomizedSearchCV(regressor, param_distributions\u001b[39m=\u001b[39mparameters ,random_state\u001b[39m=\u001b[39m\u001b[39m43\u001b[39m, verbose\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m, n_iter\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, cv\u001b[39m=\u001b[39msss, scoring\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mneg_mean_absolute_error\u001b[39m\u001b[39m\"\u001b[39m, n_jobs\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/oscarmarcussen/NTNU/ML/tdt4173-machine-learning/MLPRegressor.ipynb#X10sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# perform the search\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/oscarmarcussen/NTNU/ML/tdt4173-machine-learning/MLPRegressor.ipynb#X10sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m search \u001b[39m=\u001b[39m clf\u001b[39m.\u001b[39mfit(trainsetX, trainsetY\u001b[39m.\u001b[39mvalues\u001b[39m.\u001b[39mravel())\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/oscarmarcussen/NTNU/ML/tdt4173-machine-learning/MLPRegressor.ipynb#X10sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# report the best results\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/oscarmarcussen/NTNU/ML/tdt4173-machine-learning/MLPRegressor.ipynb#X10sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mreturn\u001b[39;00m search\u001b[39m.\u001b[39mbest_params_\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m   1146\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m   1147\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1149\u001b[0m     )\n\u001b[1;32m   1150\u001b[0m ):\n\u001b[0;32m-> 1151\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_search.py:898\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    892\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n\u001b[1;32m    893\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    894\u001b[0m     )\n\u001b[1;32m    896\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[0;32m--> 898\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_search(evaluate_candidates)\n\u001b[1;32m    900\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    901\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    902\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_search.py:1806\u001b[0m, in \u001b[0;36mRandomizedSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1804\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_search\u001b[39m(\u001b[39mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1805\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1806\u001b[0m     evaluate_candidates(\n\u001b[1;32m   1807\u001b[0m         ParameterSampler(\n\u001b[1;32m   1808\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparam_distributions, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_iter, random_state\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrandom_state\n\u001b[1;32m   1809\u001b[0m         )\n\u001b[1;32m   1810\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_search.py:845\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    838\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[1;32m    839\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFitting \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m folds for each of \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m candidates,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    840\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m totalling \u001b[39m\u001b[39m{2}\u001b[39;00m\u001b[39m fits\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    841\u001b[0m             n_splits, n_candidates, n_candidates \u001b[39m*\u001b[39m n_splits\n\u001b[1;32m    842\u001b[0m         )\n\u001b[1;32m    843\u001b[0m     )\n\u001b[0;32m--> 845\u001b[0m out \u001b[39m=\u001b[39m parallel(\n\u001b[1;32m    846\u001b[0m     delayed(_fit_and_score)(\n\u001b[1;32m    847\u001b[0m         clone(base_estimator),\n\u001b[1;32m    848\u001b[0m         X,\n\u001b[1;32m    849\u001b[0m         y,\n\u001b[1;32m    850\u001b[0m         train\u001b[39m=\u001b[39mtrain,\n\u001b[1;32m    851\u001b[0m         test\u001b[39m=\u001b[39mtest,\n\u001b[1;32m    852\u001b[0m         parameters\u001b[39m=\u001b[39mparameters,\n\u001b[1;32m    853\u001b[0m         split_progress\u001b[39m=\u001b[39m(split_idx, n_splits),\n\u001b[1;32m    854\u001b[0m         candidate_progress\u001b[39m=\u001b[39m(cand_idx, n_candidates),\n\u001b[1;32m    855\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_and_score_kwargs,\n\u001b[1;32m    856\u001b[0m     )\n\u001b[1;32m    857\u001b[0m     \u001b[39mfor\u001b[39;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[39min\u001b[39;00m product(\n\u001b[1;32m    858\u001b[0m         \u001b[39menumerate\u001b[39m(candidate_params), \u001b[39menumerate\u001b[39m(cv\u001b[39m.\u001b[39msplit(X, y, groups))\n\u001b[1;32m    859\u001b[0m     )\n\u001b[1;32m    860\u001b[0m )\n\u001b[1;32m    862\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    863\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    864\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo fits were performed. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    865\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWas the CV iterator empty? \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    866\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWere there no candidates?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    867\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     60\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[1;32m     61\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[1;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     63\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[1;32m     64\u001b[0m )\n\u001b[0;32m---> 65\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/joblib/parallel.py:1098\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1095\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1097\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend\u001b[39m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1098\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mretrieve()\n\u001b[1;32m   1099\u001b[0m \u001b[39m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m elapsed_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_start_time\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/joblib/parallel.py:975\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    973\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    974\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, \u001b[39m'\u001b[39m\u001b[39msupports_timeout\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m--> 975\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39mget(timeout\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtimeout))\n\u001b[1;32m    976\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    977\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39mget())\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/joblib/_parallel_backends.py:567\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[1;32m    565\u001b[0m \u001b[39mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[1;32m    566\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 567\u001b[0m     \u001b[39mreturn\u001b[39;00m future\u001b[39m.\u001b[39mresult(timeout\u001b[39m=\u001b[39mtimeout)\n\u001b[1;32m    568\u001b[0m \u001b[39mexcept\u001b[39;00m CfTimeoutError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    569\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/concurrent/futures/_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[1;32m    449\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__get_result()\n\u001b[0;32m--> 451\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_condition\u001b[39m.\u001b[39mwait(timeout)\n\u001b[1;32m    453\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[1;32m    454\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         waiter\u001b[39m.\u001b[39macquire()\n\u001b[1;32m    321\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "regressor = MLPRegressor(random_state=43, max_iter=1500, activation=\"logistic\", solver=\"adam\")\n",
    "\n",
    "# parameters = {'hidden_layer_sizes':((8,), (16, 8,), (16,), (32, 16,), (32,), (64, 32), (64,))}\n",
    "\n",
    "parameters = {'hidden_layer_sizes':((256, 128), (256, 128, 64))}\n",
    "\n",
    "results = searchBestFeatures(regressor, parameters)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results\n",
    "\n",
    "The best layer sizes were (64, 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the best activation and solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# updating the regressor with the best parameters\n",
    "regressor = MLPRegressor(random_state=43, max_iter=1500, hidden_layer_sizes=(64, 32))\n",
    "\n",
    "parameters = {'activation':('identity', 'logistic', 'tanh', 'relu'), 'solver':('adam',)}\n",
    "\n",
    "results = searchBestFeatures(regressor, parameters)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results\n",
    "\n",
    "The best combination was activation=logistic and solver=adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# updating the regressor with the best parameters\n",
    "model = MLPRegressor(random_state=43, max_iter=1500, hidden_layer_sizes=(258, 128), activation=\"logistic\", solver=\"adam\")\n",
    "\n",
    "fittedModel = model.fit(trainsetX, trainsetY.values.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model-evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entire:\n",
      "94.35641677739947\n",
      "may june july:\n",
      "189.53467374689265\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Print results for entire dataset\n",
    "print(\"entire:\")\n",
    "results = mean_absolute_error(fittedModel.predict(evalsetX), evalsetY)\n",
    "print(results)\n",
    "\n",
    "# Print results for May, June and July\n",
    "print(\"may june july:\")\n",
    "resultsMayJuneJuly = mean_absolute_error(fittedModel.predict(evalsetX[evalsetIndexMayJuneJuly]), evalsetY[evalsetIndexMayJuneJuly])\n",
    "print(resultsMayJuneJuly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "258,128 - logistic: 94.36, 189.5\n",
    "\n",
    "150,150 - logistic: 98.7, 197.1\n",
    "\n",
    "258,128 - relu: 114, 218.7\n",
    "\n",
    "258,128 - tanh: 102.1, 198.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean test-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_1/my8qv4fj7rx474cf5cw329lm0000gn/T/ipykernel_75017/2322883657.py:73: PerformanceWarning: Adding/subtracting object-dtype array to TimedeltaArray not vectorized.\n",
      "  X_test.iloc[:,:-4] = ((X_test.iloc[:,:-4]-dataMean[:-4])/dataStd[:-4]).fillna(value=0)\n"
     ]
    }
   ],
   "source": [
    "#Read test dataset\n",
    "X_test_estimated_a = pd.read_parquet('A/X_test_estimated.parquet')\n",
    "X_test_estimated_b = pd.read_parquet('B/X_test_estimated.parquet')\n",
    "X_test_estimated_c = pd.read_parquet('C/X_test_estimated.parquet')\n",
    "\n",
    "#add location to each sample\n",
    "X_test_estimated_a[\"location\"] = \"A\"\n",
    "X_test_estimated_b[\"location\"] = \"B\"\n",
    "X_test_estimated_c[\"location\"] = \"C\"\n",
    "\n",
    "#concat all the samples and remove date_calc column\n",
    "X_test_raw = pd.concat([\n",
    "                     X_test_estimated_a,\n",
    "                     X_test_estimated_b,\n",
    "                     X_test_estimated_c])\n",
    "\n",
    "#feature indicating time between date_calc and date_forecast\n",
    "X_test_raw[\"calc_time\"] =(X_test_raw[\"date_forecast\"] - X_test_raw[\"date_calc\"]).astype('timedelta64[s]')\n",
    "\n",
    "#fill nans\n",
    "X_test_raw[\"snow_density:kgm3\"] = X_test_raw[\"snow_density:kgm3\"].apply(\n",
    "    lambda a : np.isnan(a)\n",
    "    ).map({True: 0, False: 1})\n",
    "X_test_raw[\"ceiling_height_agl:m\"] = X_test_raw[\"ceiling_height_agl:m\"].apply(\n",
    "    lambda a : -1000 if np.isnan(a) else a\n",
    ")\n",
    "X_test_raw[\"cloud_base_agl:m\"] = X_test_raw[\"ceiling_height_agl:m\"].apply(\n",
    "    lambda a : -1000 if np.isnan(a) else a\n",
    ")\n",
    "\n",
    "#create seperate dataframes for measurments at minute 00, 15, 30 and 45\n",
    "X_test00 = X_test_raw[X_test_raw[\"date_forecast\"].apply(lambda time: time.minute == 0)].reset_index().iloc[:,1:]\n",
    "X_test15 = X_test_raw[X_test_raw[\"date_forecast\"].apply(lambda time: time.minute == 15)].reset_index().iloc[:,1:]\n",
    "X_test30 = X_test_raw[X_test_raw[\"date_forecast\"].apply(lambda time: time.minute == 30)].reset_index().iloc[:,1:]\n",
    "X_test45 = X_test_raw[X_test_raw[\"date_forecast\"].apply(lambda time: time.minute == 45)].reset_index().iloc[:,1:]\n",
    "\n",
    "#remove redundant data\n",
    "X_test15 = X_test15.iloc[:,2:-2]\n",
    "X_test30 = X_test30.iloc[:,2:-2]\n",
    "X_test45 = X_test45.iloc[:,2:-2]\n",
    "\n",
    "#join observations into single sample\n",
    "X_test_estimated = X_test00.join(X_test15, lsuffix=\"_00\", rsuffix=\"_15\").join(X_test30.join(X_test45, lsuffix=\"_30\", rsuffix=\"_45\"))\n",
    "\n",
    "#rename column for merging with targets\n",
    "X_test_estimated = X_test_estimated.rename(columns={\"date_forecast\" : \"time\"})\n",
    "\n",
    "#parse dates\n",
    "parse_dates = ['time']\n",
    "X_test_targets = pd.read_csv(\"test.csv\", parse_dates=parse_dates)\n",
    "\n",
    "#merge weatherfeatures with corresponding target pv measurement\n",
    "X_test = pd.merge(X_test_estimated, X_test_targets, on=[\"time\", \"location\"], how=\"right\").iloc[:,:-2]\n",
    "\n",
    "#add day and hour feature columns\n",
    "X_test[\"day\"] = X_test[\"time\"].dt.day_of_year\n",
    "X_test[\"hour\"] = X_test[\"time\"].dt.hour\n",
    "\n",
    "# Do not include the data because it could overfit the model\n",
    "\"\"\"\n",
    "X_test[\"location_A\"] = X_test[\"location\"].apply(lambda a : a == \"A\").map({True: 1, False: 0})\n",
    "X_test[\"location_B\"] = X_test[\"location\"].apply(lambda a : a == \"B\").map({True: 1, False: 0})\n",
    "X_test[\"location_C\"] = X_test[\"location\"].apply(lambda a : a == \"C\").map({True: 1, False: 0})\n",
    "\"\"\"\n",
    "\n",
    "# Therefore also drop location column\n",
    "X_test = X_test.drop(\"location\", axis=1)\n",
    "\n",
    "#drop time and date_calc columns\n",
    "X_test = X_test.iloc[:,2:]\n",
    "\n",
    "#normalize data\n",
    "X_test.iloc[:,:-4] = ((X_test.iloc[:,:-4]-dataMean[:-4])/dataStd[:-4]).fillna(value=0)\n",
    "\n",
    "# drop features with high correlation\n",
    "X_test = X_test.drop(to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.13922708, -6.09931622,  6.10450605, ..., 32.54562509,\n",
       "        5.69187449,  3.41269792])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preds = fittedModel.predict(X_test)\n",
    "\n",
    "display(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actual prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.139227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-6.099316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>6.104506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>17.624717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>417.324748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2155</th>\n",
       "      <td>2155</td>\n",
       "      <td>41.869631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2156</th>\n",
       "      <td>2156</td>\n",
       "      <td>36.503372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2157</th>\n",
       "      <td>2157</td>\n",
       "      <td>32.545625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2158</th>\n",
       "      <td>2158</td>\n",
       "      <td>5.691874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2159</th>\n",
       "      <td>2159</td>\n",
       "      <td>3.412698</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2160 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  prediction\n",
       "0        0   -0.139227\n",
       "1        1   -6.099316\n",
       "2        2    6.104506\n",
       "3        3   17.624717\n",
       "4        4  417.324748\n",
       "...    ...         ...\n",
       "2155  2155   41.869631\n",
       "2156  2156   36.503372\n",
       "2157  2157   32.545625\n",
       "2158  2158    5.691874\n",
       "2159  2159    3.412698\n",
       "\n",
       "[2160 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_preds = pd.read_csv(\"sample_submission.csv\")\n",
    "test_preds[\"prediction\"] = fittedModel.predict(X_test)\n",
    "display(test_preds)\n",
    "test_preds.to_csv(\"MLPRegressor_1.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
