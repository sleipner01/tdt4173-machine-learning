{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-27 10:58:00.580573: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import tensorflow as tf\n",
    "%matplotlib inline\n",
    "\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option('display.max_columns', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_location(frame, location):\n",
    "    frame[\"location\"] = location\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeNaN(frame):\n",
    "    frame = frame.fillna(method=\"ffill\")\n",
    "    frame = frame.fillna(method=\"bfill\")\n",
    "    frame = frame.fillna(value=0)\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def averageFeatures(frame):\n",
    "    frame[\"date_forecast\"] = frame[\"date_forecast\"].apply(lambda a : a.replace(minute=0))\n",
    "    return frame.groupby([\"date_forecast\"])[frame.iloc[:,1:].columns].mean().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bm/37km6rb530l8y0vv0znc0zb00000gn/T/ipykernel_69750/3652912417.py:2: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  frame = frame.fillna(method=\"ffill\")\n",
      "/var/folders/bm/37km6rb530l8y0vv0znc0zb00000gn/T/ipykernel_69750/3652912417.py:3: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  frame = frame.fillna(method=\"bfill\")\n",
      "/var/folders/bm/37km6rb530l8y0vv0znc0zb00000gn/T/ipykernel_69750/3652912417.py:2: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  frame = frame.fillna(method=\"ffill\")\n",
      "/var/folders/bm/37km6rb530l8y0vv0znc0zb00000gn/T/ipykernel_69750/3652912417.py:3: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  frame = frame.fillna(method=\"bfill\")\n",
      "/var/folders/bm/37km6rb530l8y0vv0znc0zb00000gn/T/ipykernel_69750/3652912417.py:2: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  frame = frame.fillna(method=\"ffill\")\n",
      "/var/folders/bm/37km6rb530l8y0vv0znc0zb00000gn/T/ipykernel_69750/3652912417.py:3: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  frame = frame.fillna(method=\"bfill\")\n",
      "/var/folders/bm/37km6rb530l8y0vv0znc0zb00000gn/T/ipykernel_69750/3652912417.py:2: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  frame = frame.fillna(method=\"ffill\")\n",
      "/var/folders/bm/37km6rb530l8y0vv0znc0zb00000gn/T/ipykernel_69750/3652912417.py:3: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  frame = frame.fillna(method=\"bfill\")\n",
      "/var/folders/bm/37km6rb530l8y0vv0znc0zb00000gn/T/ipykernel_69750/3652912417.py:2: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  frame = frame.fillna(method=\"ffill\")\n",
      "/var/folders/bm/37km6rb530l8y0vv0znc0zb00000gn/T/ipykernel_69750/3652912417.py:3: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  frame = frame.fillna(method=\"bfill\")\n",
      "/var/folders/bm/37km6rb530l8y0vv0znc0zb00000gn/T/ipykernel_69750/3652912417.py:2: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  frame = frame.fillna(method=\"ffill\")\n",
      "/var/folders/bm/37km6rb530l8y0vv0znc0zb00000gn/T/ipykernel_69750/3652912417.py:3: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  frame = frame.fillna(method=\"bfill\")\n"
     ]
    }
   ],
   "source": [
    "train_a = pd.read_parquet('A/train_targets.parquet')\n",
    "X_train_estimated_a = pd.read_parquet('A/X_train_estimated.parquet')\n",
    "X_train_observed_a = pd.read_parquet('A/X_train_observed.parquet')\n",
    "\n",
    "train_b = pd.read_parquet('B/train_targets.parquet')\n",
    "X_train_estimated_b = pd.read_parquet('B/X_train_estimated.parquet')\n",
    "X_train_observed_b = pd.read_parquet('B/X_train_observed.parquet')\n",
    "\n",
    "train_c = pd.read_parquet('C/train_targets.parquet')\n",
    "X_train_estimated_c = pd.read_parquet('C/X_train_estimated.parquet')\n",
    "X_train_observed_c = pd.read_parquet('C/X_train_observed.parquet')\n",
    "\n",
    "train_a = add_location(train_a, \"A\")\n",
    "train_b = add_location(train_b, \"B\")\n",
    "train_c = add_location(train_c, \"C\")\n",
    "X_train_estimated_a = X_train_estimated_a.iloc[:,1:]\n",
    "X_train_estimated_b = X_train_estimated_b.iloc[:,1:]\n",
    "X_train_estimated_c = X_train_estimated_c.iloc[:,1:]\n",
    "X_train_estimated_a = add_location(averageFeatures(removeNaN(X_train_estimated_a)), \"A\")\n",
    "X_train_estimated_b = add_location(averageFeatures(removeNaN(X_train_estimated_b)), \"B\")\n",
    "X_train_estimated_c = add_location(averageFeatures(removeNaN(X_train_estimated_c)), \"C\")\n",
    "X_train_observed_a = add_location(averageFeatures(removeNaN(X_train_observed_a)), \"A\")\n",
    "X_train_observed_b = add_location(averageFeatures(removeNaN(X_train_observed_b)), \"B\")\n",
    "X_train_observed_c = add_location(averageFeatures(removeNaN(X_train_observed_c)), \"C\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Beklager ekstremt nasty kode her\n",
    "\n",
    "\n",
    "targets = pd.concat([train_a, train_b, train_c]).sample(frac=1)\n",
    "features = pd.concat([\n",
    "                     X_train_observed_a, \n",
    "                     X_train_observed_b, \n",
    "                     X_train_observed_c,\n",
    "                     X_train_estimated_a,\n",
    "                    X_train_estimated_b,\n",
    "                    X_train_estimated_c]).sample(frac=1)\n",
    "\n",
    "train_features = features.iloc[:90000,:]\n",
    "train_features = train_features.rename(columns={\"date_forecast\" : \"time\"})\n",
    "trainset = pd.merge(train_features, targets, how=\"right\", on=[\"time\",\"location\"]).dropna()\n",
    "\n",
    "eval_features = features.iloc[90000:,:]\n",
    "eval_features = eval_features.rename(columns={\"date_forecast\" : \"time\"})\n",
    "\n",
    "evalset = pd.merge(eval_features, targets, how=\"right\", on=[\"time\",\"location\"]).dropna()\n",
    "\n",
    "trainsetX = trainset.iloc[:, :-1].reset_index().iloc[:,1:]\n",
    "trainsetX[\"day\"] = trainsetX[\"time\"].dt.day_of_year\n",
    "trainsetX[\"hour\"] = trainsetX[\"time\"].dt.hour\n",
    "\n",
    "\n",
    "sample567 = trainsetX[\"time\"].apply(lambda time : time.month in[5, 6, 7])\n",
    "#display(sample_weights)\n",
    "\n",
    "\n",
    "\n",
    "trainsetX = trainsetX.iloc[:,1:]\n",
    "trainsetX[\"location\"] = trainsetX[\"location\"].map({\"A\": 0, \"B\": 1, \"C\": 2})\n",
    "\n",
    "\n",
    "dataMean = trainsetX.mean()\n",
    "dataStd = trainsetX.std()\n",
    "trainsetX = ((trainsetX-dataMean)/dataStd).fillna(value=0)\n",
    "trainsetY = trainset.iloc[:, -1:].reset_index().iloc[:,1:]\n",
    "\n",
    "\n",
    "evalsetX = evalset.iloc[:, :-1].reset_index().iloc[:, 1:]\n",
    "\n",
    "evalsetX[\"day\"] = evalsetX[\"time\"].dt.day_of_year\n",
    "evalsetX[\"hour\"] = evalsetX[\"time\"].dt.hour\n",
    "\n",
    "\n",
    "eval567 = evalsetX[\"time\"].apply(lambda time : time.month in[5, 6, 7])\n",
    "\n",
    "evalsetX = evalsetX.iloc[:,1:]\n",
    "evalsetX[\"location\"] = evalsetX[\"location\"].map({\"A\": 0, \"B\": 1, \"C\": 2})\n",
    "evalsetX = ((evalsetX-dataMean)/dataStd).fillna(value=0)\n",
    "evalsetY = evalset.iloc[:, -1:].reset_index().iloc[:, 1:]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.GaussianNoise(stddev=0.03, seed=42),\n",
    "  tf.keras.layers.Dense(40, activation=\"tanh\"),\n",
    "  tf.keras.layers.Dense(30, activation=\"relu\"),\n",
    "  tf.keras.layers.Dense(15, activation=\"relu\"),\n",
    "  tf.keras.layers.Dense(5, activation=\"relu\")\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "166/166 [==============================] - 3s 6ms/step - loss: 88.4878 - val_loss: 85.0789\n",
      "Epoch 2/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 88.5514 - val_loss: 84.9581\n",
      "Epoch 3/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 88.4875 - val_loss: 85.0324\n",
      "Epoch 4/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 88.3614 - val_loss: 84.8789\n",
      "Epoch 5/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 88.4626 - val_loss: 85.3372\n",
      "Epoch 6/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 88.3065 - val_loss: 84.9765\n",
      "Epoch 7/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 88.3685 - val_loss: 85.1215\n",
      "Epoch 8/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 88.3673 - val_loss: 85.1421\n",
      "Epoch 9/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 88.3223 - val_loss: 85.0977\n",
      "Epoch 10/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 88.1703 - val_loss: 85.3260\n",
      "Epoch 11/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 88.3378 - val_loss: 85.2415\n",
      "Epoch 12/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 88.1708 - val_loss: 85.0626\n",
      "Epoch 13/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 88.3041 - val_loss: 84.9061\n",
      "Epoch 14/300\n",
      "166/166 [==============================] - 1s 5ms/step - loss: 88.3243 - val_loss: 85.0911\n",
      "Epoch 15/300\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 88.2585 - val_loss: 84.9604\n",
      "Epoch 16/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 88.3557 - val_loss: 84.9341\n",
      "Epoch 17/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 88.2413 - val_loss: 84.9530\n",
      "Epoch 18/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 88.2140 - val_loss: 85.4019\n",
      "Epoch 19/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 88.2302 - val_loss: 85.2106\n",
      "Epoch 20/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 88.1480 - val_loss: 86.4290\n",
      "Epoch 21/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 88.2154 - val_loss: 85.1622\n",
      "Epoch 22/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 88.1096 - val_loss: 84.8377\n",
      "Epoch 23/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 88.1944 - val_loss: 84.8827\n",
      "Epoch 24/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 88.2135 - val_loss: 85.2531\n",
      "Epoch 25/300\n",
      "166/166 [==============================] - 1s 5ms/step - loss: 88.1792 - val_loss: 85.0358\n",
      "Epoch 26/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 88.1509 - val_loss: 84.9985\n",
      "Epoch 27/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 88.1554 - val_loss: 84.8527\n",
      "Epoch 28/300\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 88.0896 - val_loss: 84.8922\n",
      "Epoch 29/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 88.1204 - val_loss: 85.1427\n",
      "Epoch 30/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 88.0586 - val_loss: 84.9328\n",
      "Epoch 31/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 88.0329 - val_loss: 85.7491\n",
      "Epoch 32/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 88.0077 - val_loss: 84.8713\n",
      "Epoch 33/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 88.1237 - val_loss: 84.9027\n",
      "Epoch 34/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 87.9725 - val_loss: 85.6128\n",
      "Epoch 35/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 88.0208 - val_loss: 85.5705\n",
      "Epoch 36/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 88.0487 - val_loss: 85.0885\n",
      "Epoch 37/300\n",
      "166/166 [==============================] - 1s 5ms/step - loss: 88.1271 - val_loss: 85.1574\n",
      "Epoch 38/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 88.0391 - val_loss: 84.9361\n",
      "Epoch 39/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 87.9999 - val_loss: 84.9886\n",
      "Epoch 40/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 87.7912 - val_loss: 84.8617\n",
      "Epoch 41/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 87.8970 - val_loss: 85.7793\n",
      "Epoch 42/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 88.1127 - val_loss: 85.0765\n",
      "Epoch 43/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 87.9017 - val_loss: 85.5002\n",
      "Epoch 44/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 87.8818 - val_loss: 85.3414\n",
      "Epoch 45/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 87.9202 - val_loss: 85.1417\n",
      "Epoch 46/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 87.9242 - val_loss: 85.5409\n",
      "Epoch 47/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 87.9081 - val_loss: 85.1407\n",
      "Epoch 48/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 88.1140 - val_loss: 84.8372\n",
      "Epoch 49/300\n",
      "166/166 [==============================] - 1s 5ms/step - loss: 87.8571 - val_loss: 85.0740\n",
      "Epoch 50/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 87.8262 - val_loss: 84.9222\n",
      "Epoch 51/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 87.8930 - val_loss: 84.9537\n",
      "Epoch 52/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 87.8994 - val_loss: 85.1682\n",
      "Epoch 53/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 87.9995 - val_loss: 84.8096\n",
      "Epoch 54/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 87.8814 - val_loss: 84.9649\n",
      "Epoch 55/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 87.7859 - val_loss: 85.1269\n",
      "Epoch 56/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 87.9072 - val_loss: 86.3345\n",
      "Epoch 57/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 87.7474 - val_loss: 85.5738\n",
      "Epoch 58/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 87.7093 - val_loss: 85.0959\n",
      "Epoch 59/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 87.7095 - val_loss: 85.2399\n",
      "Epoch 60/300\n",
      "166/166 [==============================] - 1s 5ms/step - loss: 87.7286 - val_loss: 85.4064\n",
      "Epoch 61/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 87.7741 - val_loss: 85.2568\n",
      "Epoch 62/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 87.8474 - val_loss: 84.8186\n",
      "Epoch 63/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 87.8589 - val_loss: 85.1605\n",
      "Epoch 64/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 87.7250 - val_loss: 86.7884\n",
      "Epoch 65/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 87.6434 - val_loss: 85.0376\n",
      "Epoch 66/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 87.8292 - val_loss: 85.0059\n",
      "Epoch 67/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 87.7837 - val_loss: 85.4497\n",
      "Epoch 68/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 87.6408 - val_loss: 85.3193\n",
      "Epoch 69/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 87.7435 - val_loss: 85.6941\n",
      "Epoch 70/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 87.6256 - val_loss: 84.9536\n",
      "Epoch 71/300\n",
      "166/166 [==============================] - 1s 5ms/step - loss: 87.6701 - val_loss: 84.9451\n",
      "Epoch 72/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 87.6506 - val_loss: 85.0052\n",
      "Epoch 73/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 87.6324 - val_loss: 84.9819\n",
      "Epoch 74/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 87.6773 - val_loss: 84.8890\n",
      "Epoch 75/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 87.6078 - val_loss: 85.0059\n",
      "Epoch 76/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 87.7211 - val_loss: 84.8769\n",
      "Epoch 77/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 87.7233 - val_loss: 85.6050\n",
      "Epoch 78/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 87.4884 - val_loss: 84.9753\n",
      "Epoch 79/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 87.5809 - val_loss: 85.5409\n",
      "Epoch 80/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 87.5721 - val_loss: 84.9364\n",
      "Epoch 81/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 87.5510 - val_loss: 85.9294\n",
      "Epoch 82/300\n",
      "166/166 [==============================] - 1s 5ms/step - loss: 87.6062 - val_loss: 84.9900\n",
      "Epoch 83/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 87.6396 - val_loss: 85.4333\n",
      "Epoch 84/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 87.5389 - val_loss: 85.3966\n",
      "Epoch 85/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 87.4037 - val_loss: 86.6312\n",
      "Epoch 86/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 87.5463 - val_loss: 84.8240\n",
      "Epoch 87/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 87.4853 - val_loss: 85.2674\n",
      "Epoch 88/300\n",
      "166/166 [==============================] - 1s 6ms/step - loss: 87.5851 - val_loss: 85.1196\n",
      "Epoch 89/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 87.4810 - val_loss: 85.1285\n",
      "Epoch 90/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 87.3782 - val_loss: 84.7368\n",
      "Epoch 91/300\n",
      "166/166 [==============================] - 1s 5ms/step - loss: 87.3927 - val_loss: 85.0686\n",
      "Epoch 92/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 87.3477 - val_loss: 85.3996\n",
      "Epoch 93/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 87.3985 - val_loss: 85.2582\n",
      "Epoch 94/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 87.3319 - val_loss: 85.3558\n",
      "Epoch 95/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 87.4738 - val_loss: 84.7410\n",
      "Epoch 96/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 87.5247 - val_loss: 85.1193\n",
      "Epoch 97/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 87.5348 - val_loss: 84.8032\n",
      "Epoch 98/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 87.3040 - val_loss: 85.0594\n",
      "Epoch 99/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 87.3246 - val_loss: 85.2228\n",
      "Epoch 100/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 87.4393 - val_loss: 84.8870\n",
      "Epoch 101/300\n",
      "166/166 [==============================] - 1s 5ms/step - loss: 87.2200 - val_loss: 84.9317\n",
      "Epoch 102/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 87.4141 - val_loss: 84.9542\n",
      "Epoch 103/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 87.3608 - val_loss: 85.2523\n",
      "Epoch 104/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 87.3312 - val_loss: 84.8875\n",
      "Epoch 105/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 87.2612 - val_loss: 84.8620\n",
      "Epoch 106/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 87.3432 - val_loss: 85.8034\n",
      "Epoch 107/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 87.2087 - val_loss: 85.1218\n",
      "Epoch 108/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 87.3024 - val_loss: 86.2409\n",
      "Epoch 109/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 87.3833 - val_loss: 85.2179\n",
      "Epoch 110/300\n",
      "166/166 [==============================] - 1s 5ms/step - loss: 87.1975 - val_loss: 84.9619\n",
      "Epoch 111/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 87.2958 - val_loss: 85.2030\n",
      "Epoch 112/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 87.3137 - val_loss: 86.1218\n",
      "Epoch 113/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 87.2801 - val_loss: 85.3960\n",
      "Epoch 114/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 87.1557 - val_loss: 85.1001\n",
      "Epoch 115/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 87.3116 - val_loss: 85.0180\n",
      "Epoch 116/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 87.2411 - val_loss: 85.0052\n",
      "Epoch 117/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 87.3524 - val_loss: 84.9979\n",
      "Epoch 118/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 87.0537 - val_loss: 86.2184\n",
      "Epoch 119/300\n",
      "166/166 [==============================] - 1s 6ms/step - loss: 87.2421 - val_loss: 84.8909\n",
      "Epoch 120/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 87.2919 - val_loss: 85.4682\n",
      "Epoch 121/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 87.1517 - val_loss: 84.9124\n",
      "Epoch 122/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 87.2975 - val_loss: 85.9593\n",
      "Epoch 123/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 87.0074 - val_loss: 85.0257\n",
      "Epoch 124/300\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 87.0775 - val_loss: 85.4321\n",
      "Epoch 125/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 87.1302 - val_loss: 85.3481\n",
      "Epoch 126/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 87.1941 - val_loss: 85.2913\n",
      "Epoch 127/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 87.1123 - val_loss: 85.0178\n",
      "Epoch 128/300\n",
      "166/166 [==============================] - 1s 5ms/step - loss: 87.0158 - val_loss: 85.0299\n",
      "Epoch 129/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 87.1150 - val_loss: 85.3530\n",
      "Epoch 130/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 87.1674 - val_loss: 85.1023\n",
      "Epoch 131/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 86.9969 - val_loss: 85.2173\n",
      "Epoch 132/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 87.0026 - val_loss: 84.8843\n",
      "Epoch 133/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 86.8820 - val_loss: 85.2938\n",
      "Epoch 134/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 87.0637 - val_loss: 84.8620\n",
      "Epoch 135/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 87.0266 - val_loss: 84.8595\n",
      "Epoch 136/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 87.0491 - val_loss: 85.3127\n",
      "Epoch 137/300\n",
      "166/166 [==============================] - 1s 5ms/step - loss: 87.2247 - val_loss: 84.8178\n",
      "Epoch 138/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 87.0853 - val_loss: 85.0848\n",
      "Epoch 139/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 86.8842 - val_loss: 84.7979\n",
      "Epoch 140/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 86.8665 - val_loss: 85.5382\n",
      "Epoch 141/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 86.9613 - val_loss: 85.1741\n",
      "Epoch 142/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 86.8698 - val_loss: 85.3247\n",
      "Epoch 143/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 86.8876 - val_loss: 85.1233\n",
      "Epoch 144/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 87.0046 - val_loss: 85.0214\n",
      "Epoch 145/300\n",
      "166/166 [==============================] - 1s 5ms/step - loss: 86.8571 - val_loss: 85.0959\n",
      "Epoch 146/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 86.8527 - val_loss: 84.9681\n",
      "Epoch 147/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 86.9152 - val_loss: 85.2042\n",
      "Epoch 148/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 86.8434 - val_loss: 85.4078\n",
      "Epoch 149/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 86.7085 - val_loss: 84.9715\n",
      "Epoch 150/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 86.9405 - val_loss: 84.8519\n",
      "Epoch 151/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 86.9854 - val_loss: 85.0384\n",
      "Epoch 152/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 86.9065 - val_loss: 85.2386\n",
      "Epoch 153/300\n",
      "166/166 [==============================] - 1s 5ms/step - loss: 86.8946 - val_loss: 85.1493\n",
      "Epoch 154/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 86.9863 - val_loss: 84.8595\n",
      "Epoch 155/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 86.8664 - val_loss: 85.1230\n",
      "Epoch 156/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 86.7802 - val_loss: 84.8826\n",
      "Epoch 157/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 86.8140 - val_loss: 85.9935\n",
      "Epoch 158/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 86.8198 - val_loss: 85.0965\n",
      "Epoch 159/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 86.9874 - val_loss: 84.9405\n",
      "Epoch 160/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 86.8308 - val_loss: 85.5272\n",
      "Epoch 161/300\n",
      "166/166 [==============================] - 1s 5ms/step - loss: 86.8026 - val_loss: 85.0237\n",
      "Epoch 162/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 86.8289 - val_loss: 85.8341\n",
      "Epoch 163/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 86.7544 - val_loss: 85.0673\n",
      "Epoch 164/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 86.8635 - val_loss: 84.9566\n",
      "Epoch 165/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 86.7577 - val_loss: 85.5681\n",
      "Epoch 166/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 86.6801 - val_loss: 84.9595\n",
      "Epoch 167/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 86.5473 - val_loss: 85.7524\n",
      "Epoch 168/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 86.5782 - val_loss: 85.0778\n",
      "Epoch 169/300\n",
      "166/166 [==============================] - 1s 5ms/step - loss: 86.7772 - val_loss: 85.3499\n",
      "Epoch 170/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 86.6268 - val_loss: 85.6297\n",
      "Epoch 171/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 86.6322 - val_loss: 84.9095\n",
      "Epoch 172/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 86.6911 - val_loss: 84.7894\n",
      "Epoch 173/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 86.7041 - val_loss: 84.8791\n",
      "Epoch 174/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 86.6479 - val_loss: 85.2639\n",
      "Epoch 175/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 86.6066 - val_loss: 85.5395\n",
      "Epoch 176/300\n",
      "166/166 [==============================] - 1s 5ms/step - loss: 86.5975 - val_loss: 85.9955\n",
      "Epoch 177/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 86.5853 - val_loss: 85.2175\n",
      "Epoch 178/300\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 86.5845 - val_loss: 85.0007\n",
      "Epoch 179/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 86.6036 - val_loss: 85.0855\n",
      "Epoch 180/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 86.3697 - val_loss: 85.2713\n",
      "Epoch 181/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 86.5744 - val_loss: 85.6368\n",
      "Epoch 182/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 86.4753 - val_loss: 84.9750\n",
      "Epoch 183/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 86.5712 - val_loss: 85.1073\n",
      "Epoch 184/300\n",
      "166/166 [==============================] - 1s 5ms/step - loss: 86.4938 - val_loss: 85.3761\n",
      "Epoch 185/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 86.3687 - val_loss: 85.1096\n",
      "Epoch 186/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 86.6390 - val_loss: 85.1897\n",
      "Epoch 187/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 86.3923 - val_loss: 85.0202\n",
      "Epoch 188/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 86.4139 - val_loss: 85.3256\n",
      "Epoch 189/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 86.5268 - val_loss: 85.5987\n",
      "Epoch 190/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 86.3876 - val_loss: 85.3667\n",
      "Epoch 191/300\n",
      "166/166 [==============================] - 1s 5ms/step - loss: 86.5049 - val_loss: 85.2648\n",
      "Epoch 192/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 86.3533 - val_loss: 84.9814\n",
      "Epoch 193/300\n",
      "166/166 [==============================] - 1s 5ms/step - loss: 86.3656 - val_loss: 85.4577\n",
      "Epoch 194/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 86.4950 - val_loss: 85.3315\n",
      "Epoch 195/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 86.4726 - val_loss: 85.2504\n",
      "Epoch 196/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 86.4766 - val_loss: 85.5387\n",
      "Epoch 197/300\n",
      "166/166 [==============================] - 1s 5ms/step - loss: 86.4867 - val_loss: 85.4323\n",
      "Epoch 198/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 86.4459 - val_loss: 84.9053\n",
      "Epoch 199/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 86.4125 - val_loss: 85.2559\n",
      "Epoch 200/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 86.3803 - val_loss: 85.2362\n",
      "Epoch 201/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 86.5221 - val_loss: 85.1540\n",
      "Epoch 202/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 86.3591 - val_loss: 86.2714\n",
      "Epoch 203/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 86.3202 - val_loss: 85.2465\n",
      "Epoch 204/300\n",
      "166/166 [==============================] - 1s 5ms/step - loss: 86.2828 - val_loss: 84.9784\n",
      "Epoch 205/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 86.3276 - val_loss: 85.1140\n",
      "Epoch 206/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 86.3837 - val_loss: 85.2733\n",
      "Epoch 207/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 86.3339 - val_loss: 85.4255\n",
      "Epoch 208/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 86.3923 - val_loss: 84.8866\n",
      "Epoch 209/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 86.2384 - val_loss: 85.4475\n",
      "Epoch 210/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 86.3320 - val_loss: 85.4489\n",
      "Epoch 211/300\n",
      "166/166 [==============================] - 1s 5ms/step - loss: 86.3066 - val_loss: 85.0039\n",
      "Epoch 212/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 86.2868 - val_loss: 85.8553\n",
      "Epoch 213/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 86.3387 - val_loss: 85.2105\n",
      "Epoch 214/300\n",
      "166/166 [==============================] - 1s 6ms/step - loss: 86.3341 - val_loss: 84.8802\n",
      "Epoch 215/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 86.2242 - val_loss: 84.9487\n",
      "Epoch 216/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 86.1881 - val_loss: 85.0862\n",
      "Epoch 217/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 86.0660 - val_loss: 85.6965\n",
      "Epoch 218/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 86.2305 - val_loss: 85.0762\n",
      "Epoch 219/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 86.1298 - val_loss: 84.9230\n",
      "Epoch 220/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 86.2121 - val_loss: 85.7673\n",
      "Epoch 221/300\n",
      "166/166 [==============================] - 1s 5ms/step - loss: 85.9204 - val_loss: 85.1807\n",
      "Epoch 222/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 86.1879 - val_loss: 85.2001\n",
      "Epoch 223/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 86.1653 - val_loss: 85.0966\n",
      "Epoch 224/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 86.1591 - val_loss: 84.8689\n",
      "Epoch 225/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 86.0350 - val_loss: 84.9817\n",
      "Epoch 226/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 86.1263 - val_loss: 84.9959\n",
      "Epoch 227/300\n",
      "166/166 [==============================] - 1s 5ms/step - loss: 86.1450 - val_loss: 85.0058\n",
      "Epoch 228/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 85.9663 - val_loss: 85.0668\n",
      "Epoch 229/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 86.2495 - val_loss: 85.0901\n",
      "Epoch 230/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 86.0698 - val_loss: 85.4211\n",
      "Epoch 231/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 86.1079 - val_loss: 85.3846\n",
      "Epoch 232/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 86.1284 - val_loss: 85.2842\n",
      "Epoch 233/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 86.1020 - val_loss: 85.4252\n",
      "Epoch 234/300\n",
      "166/166 [==============================] - 1s 5ms/step - loss: 86.1873 - val_loss: 85.0180\n",
      "Epoch 235/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 85.9764 - val_loss: 85.1333\n",
      "Epoch 236/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 86.1087 - val_loss: 85.2103\n",
      "Epoch 237/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 85.9259 - val_loss: 85.4788\n",
      "Epoch 238/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 86.1446 - val_loss: 85.6412\n",
      "Epoch 239/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 86.1469 - val_loss: 85.6402\n",
      "Epoch 240/300\n",
      "166/166 [==============================] - 1s 5ms/step - loss: 86.0079 - val_loss: 85.0577\n",
      "Epoch 241/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 85.9642 - val_loss: 85.8345\n",
      "Epoch 242/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 85.9779 - val_loss: 85.1278\n",
      "Epoch 243/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 85.8861 - val_loss: 85.0028\n",
      "Epoch 244/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 85.9808 - val_loss: 85.4405\n",
      "Epoch 245/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 85.9183 - val_loss: 85.0330\n",
      "Epoch 246/300\n",
      "166/166 [==============================] - 1s 5ms/step - loss: 86.0855 - val_loss: 85.3957\n",
      "Epoch 247/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 85.9257 - val_loss: 85.0325\n",
      "Epoch 248/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 85.8579 - val_loss: 85.0475\n",
      "Epoch 249/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 86.0085 - val_loss: 84.9217\n",
      "Epoch 250/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 86.0273 - val_loss: 85.1128\n",
      "Epoch 251/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 85.8175 - val_loss: 86.6535\n",
      "Epoch 252/300\n",
      "166/166 [==============================] - 1s 5ms/step - loss: 85.8598 - val_loss: 85.2477\n",
      "Epoch 253/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 85.9826 - val_loss: 85.1339\n",
      "Epoch 254/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 85.8232 - val_loss: 85.0390\n",
      "Epoch 255/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 85.9579 - val_loss: 85.0302\n",
      "Epoch 256/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 85.8764 - val_loss: 85.2571\n",
      "Epoch 257/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 85.9163 - val_loss: 85.1164\n",
      "Epoch 258/300\n",
      "166/166 [==============================] - 1s 5ms/step - loss: 85.8897 - val_loss: 85.0889\n",
      "Epoch 259/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 85.8317 - val_loss: 85.3723\n",
      "Epoch 260/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 85.8194 - val_loss: 85.0991\n",
      "Epoch 261/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 85.7714 - val_loss: 85.6194\n",
      "Epoch 262/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 85.7776 - val_loss: 85.0880\n",
      "Epoch 263/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 85.6895 - val_loss: 85.5573\n",
      "Epoch 264/300\n",
      "166/166 [==============================] - 1s 5ms/step - loss: 85.9431 - val_loss: 85.1672\n",
      "Epoch 265/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 85.6893 - val_loss: 85.0609\n",
      "Epoch 266/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 85.7786 - val_loss: 85.0743\n",
      "Epoch 267/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 85.7164 - val_loss: 85.2063\n",
      "Epoch 268/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 85.6734 - val_loss: 85.1721\n",
      "Epoch 269/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 85.8205 - val_loss: 85.1448\n",
      "Epoch 270/300\n",
      "166/166 [==============================] - 1s 5ms/step - loss: 85.5729 - val_loss: 84.9667\n",
      "Epoch 271/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 85.8576 - val_loss: 85.3035\n",
      "Epoch 272/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 85.6779 - val_loss: 85.3595\n",
      "Epoch 273/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 85.7107 - val_loss: 85.1716\n",
      "Epoch 274/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 85.6529 - val_loss: 85.1428\n",
      "Epoch 275/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 85.7301 - val_loss: 85.1708\n",
      "Epoch 276/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 85.7316 - val_loss: 85.0053\n",
      "Epoch 277/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 85.7147 - val_loss: 85.5089\n",
      "Epoch 278/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 85.7868 - val_loss: 85.1274\n",
      "Epoch 279/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 85.6892 - val_loss: 85.2212\n",
      "Epoch 280/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 85.7941 - val_loss: 85.9859\n",
      "Epoch 281/300\n",
      "166/166 [==============================] - 1s 5ms/step - loss: 85.6900 - val_loss: 85.1112\n",
      "Epoch 282/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 85.5629 - val_loss: 85.1862\n",
      "Epoch 283/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 85.6063 - val_loss: 85.1210\n",
      "Epoch 284/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 85.6766 - val_loss: 85.4859\n",
      "Epoch 285/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 85.6081 - val_loss: 85.7116\n",
      "Epoch 286/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 85.5501 - val_loss: 85.1467\n",
      "Epoch 287/300\n",
      "166/166 [==============================] - 1s 5ms/step - loss: 85.5478 - val_loss: 85.2292\n",
      "Epoch 288/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 85.6838 - val_loss: 85.8144\n",
      "Epoch 289/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 85.6619 - val_loss: 85.1519\n",
      "Epoch 290/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 85.3397 - val_loss: 85.0985\n",
      "Epoch 291/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 85.5428 - val_loss: 85.0935\n",
      "Epoch 292/300\n",
      "166/166 [==============================] - 1s 5ms/step - loss: 85.5343 - val_loss: 85.2591\n",
      "Epoch 293/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 85.5657 - val_loss: 85.7978\n",
      "Epoch 294/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 85.4994 - val_loss: 85.1723\n",
      "Epoch 295/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 85.5251 - val_loss: 85.1763\n",
      "Epoch 296/300\n",
      "166/166 [==============================] - 1s 5ms/step - loss: 85.4944 - val_loss: 85.4779\n",
      "Epoch 297/300\n",
      "166/166 [==============================] - 1s 5ms/step - loss: 85.6041 - val_loss: 85.1724\n",
      "Epoch 298/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 85.5841 - val_loss: 85.3301\n",
      "Epoch 299/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 85.5494 - val_loss: 85.6509\n",
      "Epoch 300/300\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 85.5611 - val_loss: 85.4497\n"
     ]
    }
   ],
   "source": [
    "#weighting for samples from months matching the testset\n",
    "sample_weights = sample567.map({True : 1.35, False : 1})\n",
    "\n",
    "model.compile(\n",
    "  optimizer=tf.keras.optimizers.experimental.Adadelta(learning_rate=0.5),\n",
    "  loss='mean_absolute_error'\n",
    "  )\n",
    "\n",
    "history = model.fit(trainsetX, \n",
    "                    trainsetY, \n",
    "                    batch_size=500, \n",
    "                    epochs=300,\n",
    "                    validation_data=(evalsetX, evalsetY),\n",
    "                    sample_weight=sample_weights\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69/69 [==============================] - 0s 3ms/step - loss: 183.5872\n",
      "326/326 [==============================] - 1s 2ms/step - loss: 84.6936\n"
     ]
    }
   ],
   "source": [
    "\n",
    "evalset567 = evalsetX.join(evalsetY)\n",
    "evalset567 = evalset567[eval567]\n",
    "evalset567X = evalset567.iloc[:,:-1]\n",
    "evalset567Y = evalset567.iloc[:,-1:]\n",
    "\n",
    "#results only in may, june, july(since the test set only contains data from those months)\n",
    "results1 = model.evaluate(evalset567X, evalset567Y)\n",
    "\n",
    "#results for entire eval set\n",
    "results2 = model.evaluate(evalsetX, evalsetY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 1s 5ms/step\n"
     ]
    }
   ],
   "source": [
    "#lagre \n",
    "\n",
    "preds = model.predict(evalsetX)\n",
    "compare = pd.concat([evalsetY, pd.DataFrame(preds)], axis=1)\n",
    "compare.to_csv(\"test1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bm/37km6rb530l8y0vv0znc0zb00000gn/T/ipykernel_69750/3652912417.py:2: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  frame = frame.fillna(method=\"ffill\")\n",
      "/var/folders/bm/37km6rb530l8y0vv0znc0zb00000gn/T/ipykernel_69750/3652912417.py:3: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  frame = frame.fillna(method=\"bfill\")\n",
      "/var/folders/bm/37km6rb530l8y0vv0znc0zb00000gn/T/ipykernel_69750/3652912417.py:2: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  frame = frame.fillna(method=\"ffill\")\n",
      "/var/folders/bm/37km6rb530l8y0vv0znc0zb00000gn/T/ipykernel_69750/3652912417.py:3: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  frame = frame.fillna(method=\"bfill\")\n",
      "/var/folders/bm/37km6rb530l8y0vv0znc0zb00000gn/T/ipykernel_69750/3652912417.py:2: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  frame = frame.fillna(method=\"ffill\")\n",
      "/var/folders/bm/37km6rb530l8y0vv0znc0zb00000gn/T/ipykernel_69750/3652912417.py:3: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  frame = frame.fillna(method=\"bfill\")\n"
     ]
    }
   ],
   "source": [
    "#Igjen beklager for nasty kode\n",
    "\n",
    "X_test_estimated_a = pd.read_parquet('A/X_test_estimated.parquet')\n",
    "X_test_estimated_b = pd.read_parquet('A/X_test_estimated.parquet')\n",
    "X_test_estimated_c = pd.read_parquet('A/X_test_estimated.parquet')\n",
    "\n",
    "X_test_estimated_a = X_test_estimated_a.iloc[:,1:]\n",
    "X_test_estimated_b = X_test_estimated_b.iloc[:,1:]\n",
    "X_test_estimated_c = X_test_estimated_c.iloc[:,1:]\n",
    "X_test_estimated_a = add_location(averageFeatures(removeNaN(X_test_estimated_a)), \"A\")\n",
    "X_test_estimated_b = add_location(averageFeatures(removeNaN(X_test_estimated_b)), \"B\")\n",
    "X_test_estimated_c = add_location(averageFeatures(removeNaN(X_test_estimated_c)), \"C\")\n",
    "\n",
    "\n",
    "X_test_estimated = pd.concat([X_test_estimated_a, X_test_estimated_b,X_test_estimated_c]).rename(columns={\"date_forecast\" : \"time\"})\n",
    "\n",
    "\n",
    "X_test_estimated_a[\"location\"] = \"A\"\n",
    "X_test_estimated_b[\"location\"] = \"B\"\n",
    "X_test_estimated_c[\"location\"] = \"C\"\n",
    "\n",
    "parse_dates = ['time']\n",
    "X_test_targets = pd.read_csv(\"test.csv\", parse_dates=parse_dates)\n",
    "\n",
    "X_test = pd.merge(X_test_estimated, X_test_targets, on=[\"time\", \"location\"], how=\"right\")\n",
    "\n",
    "X_test = X_test.iloc[:,0:-2]\n",
    "X_test[\"day\"] = X_test[\"time\"].dt.day_of_year\n",
    "X_test[\"hour\"] = X_test[\"time\"].dt.hour\n",
    "X_test = X_test.iloc[:,1:]\n",
    "X_test[\"location\"] = X_test[\"location\"].map({\"A\": 0, \"B\": 1, \"C\": 2})\n",
    "X_test = ((X_test-dataMean)/dataStd).fillna(value=0)\n",
    "\n",
    "X_test[\"snow_density:kgm3\"] = 0\n",
    "\n",
    "compare = pd.concat([X_test, evalsetX])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>absolute_humidity_2m:gm3</th>\n",
       "      <th>air_density_2m:kgm3</th>\n",
       "      <th>ceiling_height_agl:m</th>\n",
       "      <th>clear_sky_energy_1h:J</th>\n",
       "      <th>clear_sky_rad:W</th>\n",
       "      <th>cloud_base_agl:m</th>\n",
       "      <th>dew_or_rime:idx</th>\n",
       "      <th>dew_point_2m:K</th>\n",
       "      <th>diffuse_rad:W</th>\n",
       "      <th>diffuse_rad_1h:J</th>\n",
       "      <th>direct_rad:W</th>\n",
       "      <th>direct_rad_1h:J</th>\n",
       "      <th>effective_cloud_cover:p</th>\n",
       "      <th>elevation:m</th>\n",
       "      <th>fresh_snow_12h:cm</th>\n",
       "      <th>fresh_snow_1h:cm</th>\n",
       "      <th>fresh_snow_24h:cm</th>\n",
       "      <th>fresh_snow_3h:cm</th>\n",
       "      <th>fresh_snow_6h:cm</th>\n",
       "      <th>is_day:idx</th>\n",
       "      <th>is_in_shadow:idx</th>\n",
       "      <th>msl_pressure:hPa</th>\n",
       "      <th>precip_5min:mm</th>\n",
       "      <th>precip_type_5min:idx</th>\n",
       "      <th>pressure_100m:hPa</th>\n",
       "      <th>pressure_50m:hPa</th>\n",
       "      <th>prob_rime:p</th>\n",
       "      <th>rain_water:kgm2</th>\n",
       "      <th>relative_humidity_1000hPa:p</th>\n",
       "      <th>sfc_pressure:hPa</th>\n",
       "      <th>snow_density:kgm3</th>\n",
       "      <th>snow_depth:cm</th>\n",
       "      <th>snow_drift:idx</th>\n",
       "      <th>snow_melt_10min:mm</th>\n",
       "      <th>snow_water:kgm2</th>\n",
       "      <th>sun_azimuth:d</th>\n",
       "      <th>sun_elevation:d</th>\n",
       "      <th>super_cooled_liquid_water:kgm2</th>\n",
       "      <th>t_1000hPa:K</th>\n",
       "      <th>total_cloud_cover:p</th>\n",
       "      <th>visibility:m</th>\n",
       "      <th>wind_speed_10m:ms</th>\n",
       "      <th>wind_speed_u_10m:ms</th>\n",
       "      <th>wind_speed_v_10m:ms</th>\n",
       "      <th>wind_speed_w_1000hPa:ms</th>\n",
       "      <th>location</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>-1601.000000</td>\n",
       "      <td>-1601.000000</td>\n",
       "      <td>-1601.000000</td>\n",
       "      <td>-1601.000000</td>\n",
       "      <td>-1601.000000</td>\n",
       "      <td>-1601.000000</td>\n",
       "      <td>-1601.000000</td>\n",
       "      <td>-1601.000000</td>\n",
       "      <td>-1601.000000</td>\n",
       "      <td>-1601.000000</td>\n",
       "      <td>-1601.000000</td>\n",
       "      <td>-1601.000000</td>\n",
       "      <td>-1601.000000</td>\n",
       "      <td>-1601.000000</td>\n",
       "      <td>-1601.000000</td>\n",
       "      <td>-1601.000000</td>\n",
       "      <td>-1601.000000</td>\n",
       "      <td>-1601.000000</td>\n",
       "      <td>-1601.000000</td>\n",
       "      <td>-1601.000000</td>\n",
       "      <td>-1601.000000</td>\n",
       "      <td>-1601.000000</td>\n",
       "      <td>-1601.000000</td>\n",
       "      <td>-1601.000000</td>\n",
       "      <td>-1601.000000</td>\n",
       "      <td>-1601.000000</td>\n",
       "      <td>-1601.000000</td>\n",
       "      <td>-1601.000000</td>\n",
       "      <td>-1601.000000</td>\n",
       "      <td>-1601.000000</td>\n",
       "      <td>-1601.0</td>\n",
       "      <td>-1601.000000</td>\n",
       "      <td>-1.601000e+03</td>\n",
       "      <td>-1601.000000</td>\n",
       "      <td>-1601.000000</td>\n",
       "      <td>-1601.000000</td>\n",
       "      <td>-1601.000000</td>\n",
       "      <td>-1601.000000</td>\n",
       "      <td>-1601.000000</td>\n",
       "      <td>-1601.000000</td>\n",
       "      <td>-1601.000000</td>\n",
       "      <td>-1601.000000</td>\n",
       "      <td>-1601.000000</td>\n",
       "      <td>-1601.000000</td>\n",
       "      <td>-1601.000000</td>\n",
       "      <td>-1601.000000</td>\n",
       "      <td>-1601.000000</td>\n",
       "      <td>-1601.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.521573</td>\n",
       "      <td>-1.261044</td>\n",
       "      <td>-0.091874</td>\n",
       "      <td>0.721157</td>\n",
       "      <td>0.719155</td>\n",
       "      <td>-0.092815</td>\n",
       "      <td>0.275442</td>\n",
       "      <td>1.546494</td>\n",
       "      <td>0.493055</td>\n",
       "      <td>0.499370</td>\n",
       "      <td>0.360873</td>\n",
       "      <td>0.365657</td>\n",
       "      <td>0.123233</td>\n",
       "      <td>-0.701595</td>\n",
       "      <td>-0.355023</td>\n",
       "      <td>-0.186837</td>\n",
       "      <td>-0.460301</td>\n",
       "      <td>-0.226857</td>\n",
       "      <td>-0.271992</td>\n",
       "      <td>0.485721</td>\n",
       "      <td>-0.528910</td>\n",
       "      <td>0.319218</td>\n",
       "      <td>0.198926</td>\n",
       "      <td>0.048791</td>\n",
       "      <td>0.394911</td>\n",
       "      <td>0.377695</td>\n",
       "      <td>-0.341711</td>\n",
       "      <td>0.317980</td>\n",
       "      <td>0.240611</td>\n",
       "      <td>0.360465</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.156648</td>\n",
       "      <td>8.673617e-19</td>\n",
       "      <td>-0.238143</td>\n",
       "      <td>-0.001963</td>\n",
       "      <td>-0.003121</td>\n",
       "      <td>0.643898</td>\n",
       "      <td>0.409738</td>\n",
       "      <td>1.443917</td>\n",
       "      <td>0.150280</td>\n",
       "      <td>0.218347</td>\n",
       "      <td>0.089828</td>\n",
       "      <td>0.686640</td>\n",
       "      <td>-0.291306</td>\n",
       "      <td>0.046455</td>\n",
       "      <td>0.103442</td>\n",
       "      <td>0.568479</td>\n",
       "      <td>-0.000749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.319175</td>\n",
       "      <td>0.156757</td>\n",
       "      <td>0.053145</td>\n",
       "      <td>0.392134</td>\n",
       "      <td>0.391666</td>\n",
       "      <td>-0.196297</td>\n",
       "      <td>-0.304053</td>\n",
       "      <td>-0.081752</td>\n",
       "      <td>0.200449</td>\n",
       "      <td>0.197907</td>\n",
       "      <td>0.375222</td>\n",
       "      <td>0.376307</td>\n",
       "      <td>-0.086919</td>\n",
       "      <td>-1.009066</td>\n",
       "      <td>-1.225272</td>\n",
       "      <td>-1.054084</td>\n",
       "      <td>-1.322855</td>\n",
       "      <td>-1.064185</td>\n",
       "      <td>-1.084564</td>\n",
       "      <td>-0.201166</td>\n",
       "      <td>-0.168538</td>\n",
       "      <td>-0.331339</td>\n",
       "      <td>0.716886</td>\n",
       "      <td>-0.205276</td>\n",
       "      <td>-0.327796</td>\n",
       "      <td>-0.322130</td>\n",
       "      <td>-1.678613</td>\n",
       "      <td>1.247988</td>\n",
       "      <td>-0.009491</td>\n",
       "      <td>-0.316279</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.613921</td>\n",
       "      <td>8.675626e-19</td>\n",
       "      <td>-2.194844</td>\n",
       "      <td>0.292185</td>\n",
       "      <td>0.054323</td>\n",
       "      <td>-0.036357</td>\n",
       "      <td>0.467787</td>\n",
       "      <td>0.234474</td>\n",
       "      <td>-0.084450</td>\n",
       "      <td>0.107811</td>\n",
       "      <td>0.091899</td>\n",
       "      <td>-0.024375</td>\n",
       "      <td>-0.171761</td>\n",
       "      <td>-0.933963</td>\n",
       "      <td>0.011583</td>\n",
       "      <td>0.007839</td>\n",
       "      <td>-0.001980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.770734</td>\n",
       "      <td>-1.858154</td>\n",
       "      <td>0.009156</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001383</td>\n",
       "      <td>4.274718</td>\n",
       "      <td>1.913582</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.823280</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.880060</td>\n",
       "      <td>0.858809</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.248160</td>\n",
       "      <td>0.833788</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.015356</td>\n",
       "      <td>0.911727</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.809239</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009862</td>\n",
       "      <td>-0.028324</td>\n",
       "      <td>1.027784</td>\n",
       "      <td>0.292528</td>\n",
       "      <td>16.639623</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.534488</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.318965</td>\n",
       "      <td>-1.434606</td>\n",
       "      <td>-0.034365</td>\n",
       "      <td>0.059527</td>\n",
       "      <td>0.078629</td>\n",
       "      <td>-0.008898</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.565322</td>\n",
       "      <td>0.134428</td>\n",
       "      <td>0.159696</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>0.001795</td>\n",
       "      <td>0.551402</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.058260</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.545011</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.642196</td>\n",
       "      <td>0.629219</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.297349</td>\n",
       "      <td>0.614821</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.011172</td>\n",
       "      <td>0.617628</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.086306</td>\n",
       "      <td>0.584275</td>\n",
       "      <td>0.003155</td>\n",
       "      <td>-0.003540</td>\n",
       "      <td>0.706043</td>\n",
       "      <td>-0.132967</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.561665</td>\n",
       "      <td>0.108390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.385487</td>\n",
       "      <td>-1.021303</td>\n",
       "      <td>-0.346462</td>\n",
       "      <td>0.959782</td>\n",
       "      <td>1.135299</td>\n",
       "      <td>0.049255</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.385694</td>\n",
       "      <td>0.871392</td>\n",
       "      <td>0.804761</td>\n",
       "      <td>0.122296</td>\n",
       "      <td>0.151492</td>\n",
       "      <td>0.103066</td>\n",
       "      <td>-0.126995</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.517910</td>\n",
       "      <td>0.712929</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.770055</td>\n",
       "      <td>0.754100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.340334</td>\n",
       "      <td>0.739125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008300</td>\n",
       "      <td>0.629478</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.484111</td>\n",
       "      <td>0.130705</td>\n",
       "      <td>0.278753</td>\n",
       "      <td>0.169941</td>\n",
       "      <td>0.728386</td>\n",
       "      <td>-0.319121</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.548077</td>\n",
       "      <td>0.072260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.828200</td>\n",
       "      <td>-0.956406</td>\n",
       "      <td>0.017895</td>\n",
       "      <td>1.231856</td>\n",
       "      <td>1.340468</td>\n",
       "      <td>-0.192946</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.555244</td>\n",
       "      <td>0.654167</td>\n",
       "      <td>0.699174</td>\n",
       "      <td>0.792304</td>\n",
       "      <td>0.830307</td>\n",
       "      <td>0.004417</td>\n",
       "      <td>-2.285907</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.071641</td>\n",
       "      <td>0.092111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.167906</td>\n",
       "      <td>0.149860</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.278294</td>\n",
       "      <td>0.126695</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.310905</td>\n",
       "      <td>0.064377</td>\n",
       "      <td>0.695645</td>\n",
       "      <td>0.934144</td>\n",
       "      <td>1.499413</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.365700</td>\n",
       "      <td>0.116834</td>\n",
       "      <td>0.795416</td>\n",
       "      <td>-0.462061</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.570724</td>\n",
       "      <td>-0.108390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.192923</td>\n",
       "      <td>-1.967459</td>\n",
       "      <td>0.125562</td>\n",
       "      <td>0.542716</td>\n",
       "      <td>0.545145</td>\n",
       "      <td>0.369834</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.341705</td>\n",
       "      <td>-0.073967</td>\n",
       "      <td>0.074866</td>\n",
       "      <td>1.228765</td>\n",
       "      <td>1.280608</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.285907</td>\n",
       "      <td>-11.997418</td>\n",
       "      <td>-19.859019</td>\n",
       "      <td>-10.766852</td>\n",
       "      <td>-15.596203</td>\n",
       "      <td>-12.372566</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.909642</td>\n",
       "      <td>4.883095</td>\n",
       "      <td>-6.037648</td>\n",
       "      <td>-0.905155</td>\n",
       "      <td>-0.918366</td>\n",
       "      <td>-17.651448</td>\n",
       "      <td>12.520442</td>\n",
       "      <td>-0.070903</td>\n",
       "      <td>-0.929415</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3.858148</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-35.421463</td>\n",
       "      <td>8.498083</td>\n",
       "      <td>0.007611</td>\n",
       "      <td>0.363905</td>\n",
       "      <td>1.634752</td>\n",
       "      <td>2.578075</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.075904</td>\n",
       "      <td>0.637279</td>\n",
       "      <td>0.607733</td>\n",
       "      <td>-1.010551</td>\n",
       "      <td>-16.639623</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.579783</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       absolute_humidity_2m:gm3  air_density_2m:kgm3  ceiling_height_agl:m  \\\n",
       "count              -1601.000000         -1601.000000          -1601.000000   \n",
       "mean                   1.521573            -1.261044             -0.091874   \n",
       "std                    0.319175             0.156757              0.053145   \n",
       "min                    0.770734            -1.858154              0.009156   \n",
       "25%                    1.318965            -1.434606             -0.034365   \n",
       "50%                    1.385487            -1.021303             -0.346462   \n",
       "75%                    1.828200            -0.956406              0.017895   \n",
       "max                    2.192923            -1.967459              0.125562   \n",
       "\n",
       "       clear_sky_energy_1h:J  clear_sky_rad:W  cloud_base_agl:m  \\\n",
       "count           -1601.000000     -1601.000000      -1601.000000   \n",
       "mean                0.721157         0.719155         -0.092815   \n",
       "std                 0.392134         0.391666         -0.196297   \n",
       "min                 0.000000         0.000000          0.001383   \n",
       "25%                 0.059527         0.078629         -0.008898   \n",
       "50%                 0.959782         1.135299          0.049255   \n",
       "75%                 1.231856         1.340468         -0.192946   \n",
       "max                 0.542716         0.545145          0.369834   \n",
       "\n",
       "       dew_or_rime:idx  dew_point_2m:K  diffuse_rad:W  diffuse_rad_1h:J  \\\n",
       "count     -1601.000000    -1601.000000   -1601.000000      -1601.000000   \n",
       "mean          0.275442        1.546494       0.493055          0.499370   \n",
       "std          -0.304053       -0.081752       0.200449          0.197907   \n",
       "min           4.274718        1.913582       0.000000          0.000000   \n",
       "25%           0.000000        1.565322       0.134428          0.159696   \n",
       "50%           0.000000        1.385694       0.871392          0.804761   \n",
       "75%           0.000000        1.555244       0.654167          0.699174   \n",
       "max           0.000000        1.341705      -0.073967          0.074866   \n",
       "\n",
       "       direct_rad:W  direct_rad_1h:J  effective_cloud_cover:p  elevation:m  \\\n",
       "count  -1601.000000     -1601.000000             -1601.000000 -1601.000000   \n",
       "mean       0.360873         0.365657                 0.123233    -0.701595   \n",
       "std        0.375222         0.376307                -0.086919    -1.009066   \n",
       "min        0.000000         0.000000                 0.000000     0.000000   \n",
       "25%        0.000223         0.001795                 0.551402     0.000000   \n",
       "50%        0.122296         0.151492                 0.103066    -0.126995   \n",
       "75%        0.792304         0.830307                 0.004417    -2.285907   \n",
       "max        1.228765         1.280608                 0.000000    -2.285907   \n",
       "\n",
       "       fresh_snow_12h:cm  fresh_snow_1h:cm  fresh_snow_24h:cm  \\\n",
       "count       -1601.000000      -1601.000000       -1601.000000   \n",
       "mean           -0.355023         -0.186837          -0.460301   \n",
       "std            -1.225272         -1.054084          -1.322855   \n",
       "min             0.000000          0.000000           0.000000   \n",
       "25%             0.000000          0.000000           0.000000   \n",
       "50%             0.000000          0.000000           0.000000   \n",
       "75%             0.000000          0.000000           0.000000   \n",
       "max           -11.997418        -19.859019         -10.766852   \n",
       "\n",
       "       fresh_snow_3h:cm  fresh_snow_6h:cm   is_day:idx  is_in_shadow:idx  \\\n",
       "count      -1601.000000      -1601.000000 -1601.000000      -1601.000000   \n",
       "mean          -0.226857         -0.271992     0.485721         -0.528910   \n",
       "std           -1.064185         -1.084564    -0.201166         -0.168538   \n",
       "min            0.000000          0.000000     0.000000          0.000000   \n",
       "25%            0.000000          0.000000     2.058260          0.000000   \n",
       "50%            0.000000          0.000000     0.000000         -0.517910   \n",
       "75%            0.000000          0.000000     0.000000         -2.071641   \n",
       "max          -15.596203        -12.372566     0.000000          0.000000   \n",
       "\n",
       "       msl_pressure:hPa  precip_5min:mm  precip_type_5min:idx  \\\n",
       "count      -1601.000000    -1601.000000          -1601.000000   \n",
       "mean           0.319218        0.198926              0.048791   \n",
       "std           -0.331339        0.716886             -0.205276   \n",
       "min            0.823280        0.000000              0.000000   \n",
       "25%            0.545011        0.000000              0.000000   \n",
       "50%            0.712929        0.000000              0.000000   \n",
       "75%            0.092111        0.000000              0.000000   \n",
       "max           -0.909642        4.883095             -6.037648   \n",
       "\n",
       "       pressure_100m:hPa  pressure_50m:hPa  prob_rime:p  rain_water:kgm2  \\\n",
       "count       -1601.000000      -1601.000000 -1601.000000     -1601.000000   \n",
       "mean            0.394911          0.377695    -0.341711         0.317980   \n",
       "std            -0.327796         -0.322130    -1.678613         1.247988   \n",
       "min             0.880060          0.858809     0.000000         0.000000   \n",
       "25%             0.642196          0.629219     0.000000         0.000000   \n",
       "50%             0.770055          0.754100     0.000000         0.000000   \n",
       "75%             0.167906          0.149860     0.000000         0.000000   \n",
       "max            -0.905155         -0.918366   -17.651448        12.520442   \n",
       "\n",
       "       relative_humidity_1000hPa:p  sfc_pressure:hPa  snow_density:kgm3  \\\n",
       "count                 -1601.000000      -1601.000000            -1601.0   \n",
       "mean                      0.240611          0.360465                0.0   \n",
       "std                      -0.009491         -0.316279                0.0   \n",
       "min                       0.248160          0.833788                0.0   \n",
       "25%                       0.297349          0.614821                0.0   \n",
       "50%                       0.340334          0.739125                0.0   \n",
       "75%                       0.278294          0.126695                0.0   \n",
       "max                      -0.070903         -0.929415                0.0   \n",
       "\n",
       "       snow_depth:cm  snow_drift:idx  snow_melt_10min:mm  snow_water:kgm2  \\\n",
       "count   -1601.000000   -1.601000e+03        -1601.000000     -1601.000000   \n",
       "mean       -0.156648    8.673617e-19           -0.238143        -0.001963   \n",
       "std        -0.613921    8.675626e-19           -2.194844         0.292185   \n",
       "min         0.000000    0.000000e+00            0.000000         0.000000   \n",
       "25%         0.000000    0.000000e+00            0.000000         0.000000   \n",
       "50%         0.000000    0.000000e+00            0.000000         0.000000   \n",
       "75%         0.000000    0.000000e+00            0.000000        -0.310905   \n",
       "max        -3.858148    0.000000e+00          -35.421463         8.498083   \n",
       "\n",
       "       sun_azimuth:d  sun_elevation:d  super_cooled_liquid_water:kgm2  \\\n",
       "count   -1601.000000     -1601.000000                    -1601.000000   \n",
       "mean       -0.003121         0.643898                        0.409738   \n",
       "std         0.054323        -0.036357                        0.467787   \n",
       "min        -0.015356         0.911727                        0.000000   \n",
       "25%        -0.011172         0.617628                        0.000000   \n",
       "50%         0.008300         0.629478                        0.000000   \n",
       "75%         0.064377         0.695645                        0.934144   \n",
       "max         0.007611         0.363905                        1.634752   \n",
       "\n",
       "       t_1000hPa:K  total_cloud_cover:p  visibility:m  wind_speed_10m:ms  \\\n",
       "count -1601.000000         -1601.000000  -1601.000000       -1601.000000   \n",
       "mean      1.443917             0.150280      0.218347           0.089828   \n",
       "std       0.234474            -0.084450      0.107811           0.091899   \n",
       "min       1.809239             0.000000      0.009862          -0.028324   \n",
       "25%       1.086306             0.584275      0.003155          -0.003540   \n",
       "50%       1.484111             0.130705      0.278753           0.169941   \n",
       "75%       1.499413             0.000000      0.365700           0.116834   \n",
       "max       2.578075             0.000000     -0.075904           0.637279   \n",
       "\n",
       "       wind_speed_u_10m:ms  wind_speed_v_10m:ms  wind_speed_w_1000hPa:ms  \\\n",
       "count         -1601.000000         -1601.000000             -1601.000000   \n",
       "mean              0.686640            -0.291306                 0.046455   \n",
       "std              -0.024375            -0.171761                -0.933963   \n",
       "min               1.027784             0.292528                16.639623   \n",
       "25%               0.706043            -0.132967                 0.000000   \n",
       "50%               0.728386            -0.319121                 0.000000   \n",
       "75%               0.795416            -0.462061                 0.000000   \n",
       "max               0.607733            -1.010551               -16.639623   \n",
       "\n",
       "          location          day         hour  \n",
       "count -1601.000000 -1601.000000 -1601.000000  \n",
       "mean      0.103442     0.568479    -0.000749  \n",
       "std       0.011583     0.007839    -0.001980  \n",
       "min       0.000000     0.534488     0.000000  \n",
       "25%       0.000000     0.561665     0.108390  \n",
       "50%       0.000000     0.548077     0.072260  \n",
       "75%       0.000000     0.570724    -0.108390  \n",
       "max       0.000000     0.579783     0.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(X_test.describe()-evalsetX.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_preds = pd.read_csv(\"sample_submission.csv\")\n",
    "test_preds[\"prediction\"] = model.predict(X_test)\n",
    "test_preds.to_csv(\"preds/NN_locationTimeInput_Ada_correctNormalization_sampleWeightedByMonth.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
