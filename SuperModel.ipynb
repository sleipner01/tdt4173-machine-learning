{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Super Model (feat. Charlie)\n",
    "\n",
    "New model starting from the ground up, following ML practices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets for all locations\n",
    "X_train_observed_A_df = pd.read_csv('A/X_train_observed.csv', parse_dates=['date_forecast'], index_col='date_forecast')\n",
    "train_targets_A_df = pd.read_csv('A/train_targets.csv', parse_dates=['time'], index_col='time')\n",
    "\n",
    "X_train_observed_B_df = pd.read_csv('B/X_train_observed_B.csv', parse_dates=['date_forecast'], index_col='date_forecast')\n",
    "train_targets_B_df = pd.read_csv('B/train_targets_B.csv', parse_dates=['time'], index_col='time')\n",
    "\n",
    "X_train_observed_C_df = pd.read_csv('C/X_train_observed_C.csv', parse_dates=['date_forecast'], index_col='date_forecast')\n",
    "train_targets_C_df = pd.read_csv('C/train_targets_C.csv', parse_dates=['time'], index_col='time')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resample to hourly frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample observed data to hourly frequency\n",
    "X_train_observed_A_hourly_df = X_train_observed_A_df.resample('H').mean()\n",
    "X_train_observed_B_hourly_df = X_train_observed_B_df.resample('H').mean()\n",
    "X_train_observed_C_hourly_df = X_train_observed_C_df.resample('H').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine target values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Align the target data to the resampled observed data\n",
    "combined_A_df = X_train_observed_A_hourly_df.join(train_targets_A_df, how='inner')\n",
    "combined_B_df = X_train_observed_B_hourly_df.join(train_targets_B_df, how='inner')\n",
    "combined_C_df = X_train_observed_C_hourly_df.join(train_targets_C_df, how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values for location A\n",
    "combined_A_df['snow_density:kgm3'].fillna(0, inplace=True)\n",
    "combined_A_df.fillna(combined_A_df.mean(), inplace=True)\n",
    "\n",
    "# Handle missing values for location A\n",
    "combined_B_df['snow_density:kgm3'].fillna(0, inplace=True)\n",
    "combined_B_df.fillna(combined_B_df.mean(), inplace=True)\n",
    "\n",
    "# Handle missing values for location A\n",
    "combined_C_df['snow_density:kgm3'].fillna(0, inplace=True)\n",
    "combined_C_df.fillna(combined_C_df.mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confirm alignment after resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'X_train_observed_A': (29667, 45), 'train_targets_A': (29667, 1)}\n",
      "{'X_train_observed_B': (29222, 45), 'train_targets_B': (29222, 1)}\n",
      "{'X_train_observed_C': (29200, 45), 'train_targets_C': (29200, 1)}\n"
     ]
    }
   ],
   "source": [
    "# Confirm the alignment\n",
    "aligned_shapes_A = {\n",
    "    \"X_train_observed_A\": combined_A_df.drop(columns=['pv_measurement']).shape,\n",
    "    \"train_targets_A\": combined_A_df[['pv_measurement']].dropna().shape\n",
    "}\n",
    "\n",
    "aligned_shapes_B = {\n",
    "    \"X_train_observed_B\": combined_B_df.drop(columns=['pv_measurement']).shape,\n",
    "    \"train_targets_B\": combined_B_df[['pv_measurement']].dropna().shape\n",
    "}\n",
    "\n",
    "aligned_shapes_C = {\n",
    "    \"X_train_observed_C\": combined_C_df.drop(columns=['pv_measurement']).shape,\n",
    "    \"train_targets_C\": combined_C_df[['pv_measurement']].dropna().shape\n",
    "}\n",
    "\n",
    "# Output the aligned shapes\n",
    "print(aligned_shapes_A)\n",
    "print(aligned_shapes_B)\n",
    "print(aligned_shapes_C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation Analysis\n",
    "Top 10 correlated features with target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(pv_measurement           1.000000\n",
       " direct_rad:W             0.864334\n",
       " direct_rad_1h:J          0.850497\n",
       " clear_sky_rad:W          0.812047\n",
       " clear_sky_energy_1h:J    0.799407\n",
       " diffuse_rad:W            0.711396\n",
       " diffuse_rad_1h:J         0.703887\n",
       " sun_elevation:d          0.695713\n",
       " is_day:idx               0.554569\n",
       " t_1000hPa:K              0.351691\n",
       " Name: pv_measurement, dtype: float64,\n",
       " pv_measurement           1.000000\n",
       " direct_rad:W             0.813472\n",
       " direct_rad_1h:J          0.802817\n",
       " clear_sky_rad:W          0.800800\n",
       " clear_sky_energy_1h:J    0.790263\n",
       " diffuse_rad:W            0.688844\n",
       " diffuse_rad_1h:J         0.684326\n",
       " sun_elevation:d          0.660414\n",
       " is_day:idx               0.489087\n",
       " t_1000hPa:K              0.433769\n",
       " Name: pv_measurement, dtype: float64,\n",
       " pv_measurement           1.000000\n",
       " direct_rad:W             0.740432\n",
       " direct_rad_1h:J          0.738604\n",
       " clear_sky_rad:W          0.728901\n",
       " clear_sky_energy_1h:J    0.724331\n",
       " diffuse_rad:W            0.623510\n",
       " diffuse_rad_1h:J         0.621372\n",
       " sun_elevation:d          0.607788\n",
       " is_day:idx               0.464148\n",
       " t_1000hPa:K              0.378830\n",
       " Name: pv_measurement, dtype: float64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the correlation matrix for the combined dataset of location A\n",
    "correlation_matrix_A = combined_A_df.corr()\n",
    "correlation_matrix_B = combined_B_df.corr()\n",
    "correlation_matrix_C = combined_C_df.corr()\n",
    "\n",
    "# Extract the correlations of features with the solar output for location A\n",
    "feature_correlation_with_target_A = correlation_matrix_A['pv_measurement'].sort_values(ascending=False)\n",
    "feature_correlation_with_target_B = correlation_matrix_B['pv_measurement'].sort_values(ascending=False)\n",
    "feature_correlation_with_target_C = correlation_matrix_C['pv_measurement'].sort_values(ascending=False)\n",
    "\n",
    "# Output the top correlated features for location A\n",
    "top_correlated_features_A = feature_correlation_with_target_A.head(10)\n",
    "top_correlated_features_B = feature_correlation_with_target_B.head(10)\n",
    "top_correlated_features_C = feature_correlation_with_target_C.head(10)\n",
    "\n",
    "top_correlated_features_A, top_correlated_features_B, top_correlated_features_C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10 Lowest correlated features with target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(super_cooled_liquid_water:kgm2   -0.131534\n",
       " snow_water:kgm2                  -0.138618\n",
       " total_cloud_cover:p              -0.184978\n",
       " effective_cloud_cover:p          -0.217712\n",
       " wind_speed_v_10m:ms              -0.279632\n",
       " air_density_2m:kgm3              -0.376162\n",
       " relative_humidity_1000hPa:p      -0.405435\n",
       " is_in_shadow:idx                 -0.586869\n",
       " elevation:m                            NaN\n",
       " snow_drift:idx                         NaN\n",
       " Name: pv_measurement, dtype: float64,\n",
       " super_cooled_liquid_water:kgm2   -0.096350\n",
       " snow_density:kgm3                -0.113997\n",
       " snow_water:kgm2                  -0.115522\n",
       " total_cloud_cover:p              -0.142852\n",
       " effective_cloud_cover:p          -0.174269\n",
       " wind_speed_v_10m:ms              -0.273067\n",
       " relative_humidity_1000hPa:p      -0.360881\n",
       " air_density_2m:kgm3              -0.386191\n",
       " is_in_shadow:idx                 -0.521125\n",
       " elevation:m                            NaN\n",
       " Name: pv_measurement, dtype: float64,\n",
       " snow_density:kgm3             -0.095998\n",
       " prob_rime:p                   -0.104005\n",
       " total_cloud_cover:p           -0.108114\n",
       " snow_water:kgm2               -0.109671\n",
       " effective_cloud_cover:p       -0.137172\n",
       " wind_speed_v_10m:ms           -0.293387\n",
       " relative_humidity_1000hPa:p   -0.339471\n",
       " air_density_2m:kgm3           -0.383293\n",
       " is_in_shadow:idx              -0.534859\n",
       " elevation:m                         NaN\n",
       " Name: pv_measurement, dtype: float64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Output the top correlated features for location A\n",
    "low_correlated_features_A = feature_correlation_with_target_A.tail(10)\n",
    "low_correlated_features_B = feature_correlation_with_target_B.tail(10)\n",
    "low_correlated_features_C = feature_correlation_with_target_C.tail(10)\n",
    "\n",
    "low_correlated_features_A, low_correlated_features_B, low_correlated_features_C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'X_train_A_selected': (29667, 9),\n",
       " 'X_train_B_selected': (29222, 9),\n",
       " 'X_train_C_selected': (29200, 9)}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the features to keep based on the highest correlations provided earlier\n",
    "features_to_keep = [\n",
    "    'direct_rad:W', 'direct_rad_1h:J', 'clear_sky_rad:W', \n",
    "    'clear_sky_energy_1h:J', 'diffuse_rad:W', 'diffuse_rad_1h:J', \n",
    "    'sun_elevation:d', 'is_day:idx', 't_1000hPa:K'\n",
    "]\n",
    "\n",
    "# Define the features to drop based on the lowest correlations provided earlier\n",
    "# features_to_drop = [\n",
    "#     'super_cooled_liquid_water:kgm2', 'snow_water:kgm2', 'total_cloud_cover:p',\n",
    "#     'effective_cloud_cover:p', 'wind_speed_v_10m:ms', 'air_density_2m:kgm3',\n",
    "#     'relative_humidity_1000hPa:p', 'is_in_shadow:idx', 'elevation:m', \n",
    "#     'snow_drift:idx', 'snow_density:kgm3', 'prob_rime:p'\n",
    "# ]\n",
    "\n",
    "# Prepare the datasets for each location\n",
    "# We create copies of the dataframes to avoid modifying the original dataframes\n",
    "X_train_A_selected = combined_A_df[features_to_keep].copy()\n",
    "X_train_B_selected = combined_B_df[features_to_keep].copy()\n",
    "X_train_C_selected = combined_C_df[features_to_keep].copy()\n",
    "\n",
    "# Drop the features with the lowest correlations\n",
    "# X_train_A_selected.drop(columns=features_to_drop, errors='ignore', inplace=True)\n",
    "# X_train_B_selected.drop(columns=features_to_drop, errors='ignore', inplace=True)\n",
    "# X_train_C_selected.drop(columns=features_to_drop, errors='ignore', inplace=True)\n",
    "\n",
    "# Output the shapes of the prepared datasets to ensure the process was successful\n",
    "prepared_shapes = {\n",
    "    \"X_train_A_selected\": X_train_A_selected.shape,\n",
    "    \"X_train_B_selected\": X_train_B_selected.shape,\n",
    "    \"X_train_C_selected\": X_train_C_selected.shape\n",
    "}\n",
    "\n",
    "prepared_shapes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/base.py:1152: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 216133.6944851277\n",
      "RMSE: 464.9018116604061\n",
      "MAE: 215.09228178294572\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Assuming 'X_train_A_selected' is your feature set and 'train_targets_A' is your target set for location A\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_train_A_selected, combined_A_df[['pv_measurement']], test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the model - Random Forest Regressor in this case\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "print(f'MSE: {mse}')\n",
    "print(f'RMSE: {rmse}')\n",
    "print(f'MAE: {mae}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
