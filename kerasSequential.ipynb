{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import sklearn\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option('display.max_columns', 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_1/my8qv4fj7rx474cf5cw329lm0000gn/T/ipykernel_78020/3919423765.py:118: PerformanceWarning: Adding/subtracting object-dtype array to TimedeltaArray not vectorized.\n",
      "  datasetX.iloc[:,:-4] = ((datasetX.iloc[:,:-4]-dataMean[:-4])/dataStd[:-4]).fillna(value=0)\n"
     ]
    }
   ],
   "source": [
    "#Read dataset\n",
    "train_a = pd.read_parquet('A/train_targets.parquet')\n",
    "train_b = pd.read_parquet('B/train_targets.parquet')\n",
    "train_c = pd.read_parquet('C/train_targets.parquet')\n",
    "\n",
    "X_train_estimated_a = pd.read_parquet('A/X_train_estimated.parquet')\n",
    "X_train_estimated_b = pd.read_parquet('B/X_train_estimated.parquet')\n",
    "X_train_estimated_c = pd.read_parquet('C/X_train_estimated.parquet')\n",
    "\n",
    "X_train_observed_a = pd.read_parquet('A/X_train_observed.parquet')\n",
    "X_train_observed_b = pd.read_parquet('B/X_train_observed.parquet')\n",
    "X_train_observed_c = pd.read_parquet('C/X_train_observed.parquet')\n",
    "\n",
    "#add location to each sample\n",
    "train_a[\"location\"] = \"A\"\n",
    "train_b[\"location\"] = \"B\"\n",
    "train_c[\"location\"] = \"C\"\n",
    "\n",
    "X_train_estimated_a[\"location\"] = \"A\"\n",
    "X_train_estimated_b[\"location\"] = \"B\"\n",
    "X_train_estimated_c[\"location\"] = \"C\"\n",
    "\n",
    "X_train_observed_a[\"location\"] = \"A\"\n",
    "X_train_observed_b[\"location\"] = \"B\"\n",
    "X_train_observed_c[\"location\"] = \"C\"\n",
    "\n",
    "#remove extra minute 00 sample\n",
    "X_train_observed_a = X_train_observed_a.iloc[:-1,:]\n",
    "X_train_observed_b = X_train_observed_b.iloc[:-1,:]\n",
    "X_train_observed_c = X_train_observed_c.iloc[:-1,:]\n",
    "\n",
    "#add date_calc column same as date_forecast column to observed data\n",
    "X_train_observed_a.insert(0, \"date_calc\", X_train_observed_a[\"date_forecast\"])\n",
    "X_train_observed_b.insert(0, \"date_calc\", X_train_observed_b[\"date_forecast\"])\n",
    "X_train_observed_c.insert(0, \"date_calc\", X_train_observed_c[\"date_forecast\"])\n",
    "\n",
    "#concat all the samples\n",
    "X_train_raw = pd.concat([X_train_observed_a,\n",
    "                     X_train_observed_b,\n",
    "                     X_train_observed_c,\n",
    "                     X_train_estimated_a,\n",
    "                     X_train_estimated_b,\n",
    "                     X_train_estimated_c])\n",
    "\n",
    "#feature indicating time between date_calc and date_forecast\n",
    "X_train_raw[\"calc_time\"] =(X_train_raw[\"date_forecast\"] - X_train_raw[\"date_calc\"]).astype('timedelta64[s]')\n",
    "\n",
    "#fill nans\n",
    "X_train_raw[\"snow_density:kgm3\"] = X_train_raw[\"snow_density:kgm3\"].apply(\n",
    "    lambda a : np.isnan(a)\n",
    "    ).map({True: 0, False: 1})\n",
    "X_train_raw[\"ceiling_height_agl:m\"] = X_train_raw[\"ceiling_height_agl:m\"].apply(\n",
    "    lambda a : -1000 if np.isnan(a) else a\n",
    ")\n",
    "X_train_raw[\"cloud_base_agl:m\"] = X_train_raw[\"ceiling_height_agl:m\"].apply(\n",
    "    lambda a : -1000 if np.isnan(a) else a\n",
    ")\n",
    "\n",
    "#create seperate dataframes for measurments at minute 00, 15, 30 and 45\n",
    "X_train00 = X_train_raw[X_train_raw[\"date_forecast\"].apply(lambda time: time.minute == 0)].reset_index().iloc[:,1:]\n",
    "X_train15 = X_train_raw[X_train_raw[\"date_forecast\"].apply(lambda time: time.minute == 15)].reset_index().iloc[:,1:]\n",
    "X_train30 = X_train_raw[X_train_raw[\"date_forecast\"].apply(lambda time: time.minute == 30)].reset_index().iloc[:,1:]\n",
    "X_train45 = X_train_raw[X_train_raw[\"date_forecast\"].apply(lambda time: time.minute == 45)].reset_index().iloc[:,1:]\n",
    "\n",
    "#remove redundant data\n",
    "X_train15 = X_train15.iloc[:,2:-2]\n",
    "X_train30 = X_train30.iloc[:,2:-2]\n",
    "X_train45 = X_train45.iloc[:,2:-2]\n",
    "\n",
    "#join observations into single sample\n",
    "X_train = X_train00.join(X_train15, lsuffix=\"_00\", rsuffix=\"_15\").join(X_train30.join(X_train45, lsuffix=\"_30\", rsuffix=\"_45\"))\n",
    "\n",
    "#rename column for merging with targets\n",
    "X_train = X_train.rename(columns={\"date_forecast\" : \"time\"})\n",
    "\n",
    "#concat target values and drop NaN values\n",
    "targets = pd.concat([train_a,\n",
    "                     train_b,\n",
    "                     train_c]).dropna()\n",
    "\n",
    "#merge weatherfeatures with corresponding target pv measurement\n",
    "dataset = pd.merge(X_train, targets, how=\"right\", on=[\"time\", \"location\"])\n",
    "\n",
    "#shuffle dataset\n",
    "dataset = dataset.sample(frac=1, random_state=43).reset_index().iloc[:,1:]\n",
    "\n",
    "#split into features and targets\n",
    "datasetX = dataset.iloc[:, :-1]\n",
    "datasetY = dataset.iloc[:, -1:]\n",
    "\n",
    "#add day and hour feature columns\n",
    "datasetX[\"day\"] = datasetX[\"time\"].dt.day_of_year\n",
    "datasetX[\"hour\"] = datasetX[\"time\"].dt.hour\n",
    "\n",
    "#get indexes of samples in the months of the test dataset\n",
    "indexMayJuneJuly = datasetX[\"time\"].apply(lambda time : time.month in [5, 6, 7])\n",
    "\n",
    "#OHE encoding for catagorical feature \"location\"\n",
    "\n",
    "# Do not include the data because it could overfit the model\n",
    "\"\"\"\n",
    "datasetX[\"location_A\"] = datasetX[\"location\"].apply(lambda a : a == \"A\").map({True: 1, False: 0})\n",
    "datasetX[\"location_B\"] = datasetX[\"location\"].apply(lambda a : a == \"B\").map({True: 1, False: 0})\n",
    "datasetX[\"location_C\"] = datasetX[\"location\"].apply(lambda a : a == \"C\").map({True: 1, False: 0})\n",
    "\"\"\"\n",
    "\n",
    "# Therefore also drop location column\n",
    "datasetX = datasetX.drop(\"location\", axis=1)\n",
    "\n",
    "#drop time and date_calc columns\n",
    "datasetX = datasetX.iloc[:,2:]\n",
    "\n",
    "#calculate mean and std for normalizing data, values should also be used for normalizing test data\n",
    "dataMean = datasetX.mean()\n",
    "dataStd = datasetX.std()\n",
    "\n",
    "#normalize data\n",
    "datasetX.iloc[:,:-4] = ((datasetX.iloc[:,:-4]-dataMean[:-4])/dataStd[:-4]).fillna(value=0)\n",
    "\n",
    "#partition into training and evalset\n",
    "trainsetX = datasetX.iloc[:85000,:]\n",
    "trainsetY = datasetY.iloc[:85000,:]\n",
    "trainsetIndexMayJuneJuly = indexMayJuneJuly[:85000]\n",
    "evalsetX = datasetX.iloc[85000:,:]\n",
    "evalsetY = datasetY.iloc[85000:,:]\n",
    "evalsetIndexMayJuneJuly = indexMayJuneJuly[85000:]\n",
    "\n",
    "#display(datasetX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up highly correlated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create correlation matrix\n",
    "corr_matrix = trainsetX.corr().abs()\n",
    "\n",
    "# Select upper triangle of correlation matrix\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "# Find features with correlation greater than 0.75\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.75)]\n",
    "\n",
    "trainsetX = trainsetX.drop(to_drop, axis=1)\n",
    "evalsetX = evalsetX.drop(to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adadelta` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adadelta`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adadelta`.\n",
      "2023-10-30 19:38:36.125313: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_10' with dtype double and shape [85000,35]\n",
      "\t [[{{node Placeholder/_10}}]]\n",
      "2023-10-30 19:38:36.125494: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_11' with dtype double and shape [85000,1]\n",
      "\t [[{{node Placeholder/_11}}]]\n"
     ]
    }
   ],
   "source": [
    "tf.keras.utils.set_random_seed(43)\n",
    "tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "model = (tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Dense(150, activation=\"tanh\"),\n",
    "        tf.keras.layers.Dense(120, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(90, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(60, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(1, activation=\"relu\"),\n",
    "    ]))\n",
    "\n",
    "model.compile (\n",
    "         optimizer=tf.keras.optimizers.experimental.Adadelta(learning_rate=1),\n",
    "        loss=\"mean_absolute_error\"\n",
    "    )\n",
    "\n",
    "history = model.fit(x = trainsetX,\n",
    "                        y= trainsetY,\n",
    "                        batch_size = 1000,\n",
    "                        epochs=100,\n",
    "                        verbose=0,\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model-evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entire:\n",
      "251/251 [==============================] - 0s 431us/step - loss: 289.7400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-30 19:39:20.701994: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_11' with dtype double and shape [8024,1]\n",
      "\t [[{{node Placeholder/_11}}]]\n",
      "2023-10-30 19:39:20.702169: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_11' with dtype double and shape [8024,1]\n",
      "\t [[{{node Placeholder/_11}}]]\n",
      "2023-10-30 19:39:20.891002: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_11' with dtype double and shape [1640,1]\n",
      "\t [[{{node Placeholder/_11}}]]\n",
      "2023-10-30 19:39:20.891156: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_11' with dtype double and shape [1640,1]\n",
      "\t [[{{node Placeholder/_11}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "289.7400207519531\n",
      "may june july:\n",
      "52/52 [==============================] - 0s 427us/step - loss: 683.2075\n",
      "683.20751953125\n"
     ]
    }
   ],
   "source": [
    "print(\"entire:\")\n",
    "results = model.evaluate(evalsetX, evalsetY)\n",
    "print(results)\n",
    "\n",
    "print(\"may june july:\")\n",
    "resultsMayJuneJuly = model.evaluate(evalsetX[evalsetIndexMayJuneJuly], evalsetY[evalsetIndexMayJuneJuly])\n",
    "print(resultsMayJuneJuly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean test-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read test dataset\n",
    "X_test_estimated_a = pd.read_parquet('A/X_test_estimated.parquet')\n",
    "X_test_estimated_b = pd.read_parquet('B/X_test_estimated.parquet')\n",
    "X_test_estimated_c = pd.read_parquet('C/X_test_estimated.parquet')\n",
    "\n",
    "#add location to each sample\n",
    "X_test_estimated_a[\"location\"] = \"A\"\n",
    "X_test_estimated_b[\"location\"] = \"B\"\n",
    "X_test_estimated_c[\"location\"] = \"C\"\n",
    "\n",
    "#concat all the samples and remove date_calc column\n",
    "X_test_raw = pd.concat([\n",
    "                     X_test_estimated_a,\n",
    "                     X_test_estimated_b,\n",
    "                     X_test_estimated_c])\n",
    "\n",
    "#feature indicating time between date_calc and date_forecast\n",
    "X_test_raw[\"calc_time\"] =(X_test_raw[\"date_forecast\"] - X_test_raw[\"date_calc\"]).astype('timedelta64[s]')\n",
    "\n",
    "#fill nans\n",
    "X_test_raw[\"snow_density:kgm3\"] = X_test_raw[\"snow_density:kgm3\"].apply(\n",
    "    lambda a : np.isnan(a)\n",
    "    ).map({True: 0, False: 1})\n",
    "X_test_raw[\"ceiling_height_agl:m\"] = X_test_raw[\"ceiling_height_agl:m\"].apply(\n",
    "    lambda a : -1000 if np.isnan(a) else a\n",
    ")\n",
    "X_test_raw[\"cloud_base_agl:m\"] = X_test_raw[\"ceiling_height_agl:m\"].apply(\n",
    "    lambda a : -1000 if np.isnan(a) else a\n",
    ")\n",
    "\n",
    "#create seperate dataframes for measurments at minute 00, 15, 30 and 45\n",
    "X_test00 = X_test_raw[X_test_raw[\"date_forecast\"].apply(lambda time: time.minute == 0)].reset_index().iloc[:,1:]\n",
    "X_test15 = X_test_raw[X_test_raw[\"date_forecast\"].apply(lambda time: time.minute == 15)].reset_index().iloc[:,1:]\n",
    "X_test30 = X_test_raw[X_test_raw[\"date_forecast\"].apply(lambda time: time.minute == 30)].reset_index().iloc[:,1:]\n",
    "X_test45 = X_test_raw[X_test_raw[\"date_forecast\"].apply(lambda time: time.minute == 45)].reset_index().iloc[:,1:]\n",
    "\n",
    "#remove redundant data\n",
    "X_test15 = X_test15.iloc[:,2:-2]\n",
    "X_test30 = X_test30.iloc[:,2:-2]\n",
    "X_test45 = X_test45.iloc[:,2:-2]\n",
    "\n",
    "#join observations into single sample\n",
    "X_test_estimated = X_test00.join(X_test15, lsuffix=\"_00\", rsuffix=\"_15\").join(X_test30.join(X_test45, lsuffix=\"_30\", rsuffix=\"_45\"))\n",
    "\n",
    "#rename column for merging with targets\n",
    "X_test_estimated = X_test_estimated.rename(columns={\"date_forecast\" : \"time\"})\n",
    "\n",
    "#parse dates\n",
    "parse_dates = ['time']\n",
    "X_test_targets = pd.read_csv(\"test.csv\", parse_dates=parse_dates)\n",
    "\n",
    "#merge weatherfeatures with corresponding target pv measurement\n",
    "X_test = pd.merge(X_test_estimated, X_test_targets, on=[\"time\", \"location\"], how=\"right\").iloc[:,:-2]\n",
    "\n",
    "#add day and hour feature columns\n",
    "X_test[\"day\"] = X_test[\"time\"].dt.day_of_year\n",
    "X_test[\"hour\"] = X_test[\"time\"].dt.hour\n",
    "\n",
    "# Do not include the data because it could overfit the model\n",
    "\"\"\"\n",
    "X_test[\"location_A\"] = X_test[\"location\"].apply(lambda a : a == \"A\").map({True: 1, False: 0})\n",
    "X_test[\"location_B\"] = X_test[\"location\"].apply(lambda a : a == \"B\").map({True: 1, False: 0})\n",
    "X_test[\"location_C\"] = X_test[\"location\"].apply(lambda a : a == \"C\").map({True: 1, False: 0})\n",
    "\"\"\"\n",
    "\n",
    "# Therefore also drop location column\n",
    "X_test = X_test.drop(\"location\", axis=1)\n",
    "\n",
    "#drop time and date_calc columns\n",
    "X_test = X_test.iloc[:,2:]\n",
    "\n",
    "#normalize data\n",
    "X_test.iloc[:,:-4] = ((X_test.iloc[:,:-4]-dataMean[:-4])/dataStd[:-4]).fillna(value=0)\n",
    "\n",
    "#display(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!ReWrite\n",
    "def models_predict(models, X):\n",
    "    preds = X.iloc[:,1:2]\n",
    "\n",
    "    for i in range(len(models)):\n",
    "        preds[str(i)] = models[i].predict(X)\n",
    "\n",
    "    preds = preds.iloc[:,1:]\n",
    "    preds[\"final\"] = preds.mean(axis=1)\n",
    "    return preds[\"final\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actual prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = pd.read_csv(\"sample_submission.csv\")\n",
    "test_preds[\"prediction\"] = models_predict(models, X_test)\n",
    "display(test_preds)\n",
    "test_preds.to_csv(\"Ensamble2_reproduced.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
